{"./":{"url":"./","title":"Introduction","keywords":"","body":"kubernetes-notes Kubernetes 学习笔记 Summary Introduction 安装与配置 使用kubespray安装kubernetes 使用minikube安装kubernetes 基本概念 kubernetes架构 Kubernetes总架构图 基于Docker及Kubernetes技术构建容器云（PaaS）平台概述 kubernetes对象 理解kubernetes对象 kubernetes常用对象说明 Pod详解 核心原理 Api Server Controller Manager Scheduler Kubelet 运维指南 kubernetes集群问题排查 指定Node调度与隔离 监控体系 监控体系介绍 cAdvisor介绍 Heapster介绍 Influxdb介绍 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"setup/install-k8s-by-kubespray.html":{"url":"setup/install-k8s-by-kubespray.html","title":"使用kubespray安装kubernetes","keywords":"","body":"1. 环境准备 1.1. 部署机器 以下机器为虚拟机 机器IP 主机名 角色 系统版本 备注 172.16.94.140 kube-master-0 k8s master Centos 4.17.14 内存：3G 172.16.94.141 kube-node-41 k8s node Centos 4.17.14 内存：3G 172.16.94.142 kube-node-42 k8s node Centos 4.17.14 内存：3G 172.16.94.135 部署管理机 1.2. 配置管理机 管理机主要用来部署k8s集群，需要安装以下版本的软件，具体可参考： https://github.com/kubernetes-incubator/kubespray#requirements https://github.com/kubernetes-incubator/kubespray/blob/master/requirements.txt ansible>=2.4.0 jinja2>=2.9.6 netaddr pbr>=1.6 ansible-modules-hashivault>=3.9.4 hvac 1、安装及配置ansible 参考ansible的使用。 给部署机器配置SSH的免密登录权限，具体参考ssh免密登录。 2、安装python-netaddr # 安装pip yum -y install epel-release yum -y install python-pip # 安装python-netaddr pip install netaddr 3、升级Jinja # Jinja 2.9 (or newer) pip install --upgrade jinja2 1.3. 配置部署机器 部署机器即用来运行k8s集群的机器，包括Master和Node。 1、确认系统版本 本文采用centos7的系统，建议将系统内核升级到4.x.x以上。 2、关闭防火墙 systemctl stop firewalld systemctl disable firewalld iptables -F 3、关闭swap Kubespary v2.5.0的版本需要关闭swap，具体参考 https://github.com/kubernetes-incubator/kubespray/blob/02cd5418c22d51e40261775908d55bc562206023/roles/kubernetes/preinstall/tasks/verify-settings.yml#L75 - name: Stop if swap enabled assert: that: ansible_swaptotal_mb == 0 when: kubelet_fail_swap_on|default(true) ignore_errors: \"{{ ignore_assert_errors }}\" V2.6.0 版本去除了swap的检查，具体参考： https://github.com/kubernetes-incubator/kubespray/commit/b902602d161f8c147f3d155d2ac5360244577127#diff-b92ae64dd18d34a96fbeb7f7e48a6a9b 执行关闭swap命令swapoff -a。 [root@master ~]#swapoff -a [root@master ~]# [root@master ~]# free -m total used free shared buff/cache available Mem: 976 366 135 6 474 393 Swap: 0 0 0 # swap 一栏为0，表示已经关闭了swap 4、确认部署机器内存 由于本文采用虚拟机部署，内存可能存在不足的问题，因此将虚拟机内存调整为3G或以上；如果是物理机一般不会有内存不足的问题。具体参考： https://github.com/kubernetes-incubator/kubespray/blob/95f1e4634a1c50fa77312d058a2b713353f4307e/roles/kubernetes/preinstall/tasks/verify-settings.yml#L52 - name: Stop if memory is too small for masters assert: that: ansible_memtotal_mb >= 1500 ignore_errors: \"{{ ignore_assert_errors }}\" when: inventory_hostname in groups['kube-master'] - name: Stop if memory is too small for nodes assert: that: ansible_memtotal_mb >= 1024 ignore_errors: \"{{ ignore_assert_errors }}\" when: inventory_hostname in groups['kube-node'] 1.4. 涉及镜像 Docker版本为17.03.2-ce。 1、Master节点 镜像 版本 大小 镜像ID 备注 gcr.io/google-containers/hyperkube v1.9.5 620 MB a7e7fdbc5fee k8s quay.io/coreos/etcd v3.2.4 35.7 MB 498ffffcfd05 gcr.io/google_containers/pause-amd64 3.0 747 kB 99e59f495ffa quay.io/calico/node v2.6.8 282 MB e96a297310fd calico quay.io/calico/cni v1.11.4 70.8 MB 4c4cb67d7a88 calico quay.io/calico/ctl v1.6.3 44.4 MB 46d3aace8bc6 calico 2、Node节点 镜像 版本 大小 镜像ID 备注 gcr.io/google-containers/hyperkube v1.9.5 620 MB a7e7fdbc5fee k8s gcr.io/google_containers/pause-amd64 3.0 747 kB 99e59f495ffa quay.io/calico/node v2.6.8 282 MB e96a297310fd calico quay.io/calico/cni v1.11.4 70.8 MB 4c4cb67d7a88 calico quay.io/calico/ctl v1.6.3 44.4 MB 46d3aace8bc6 calico gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64 1.14.8 40.9 MB c2ce1ffb51ed dns gcr.io/google_containers/k8s-dns-sidecar-amd64 1.14.8 42.2 MB 6f7f2dc7fab5 dns gcr.io/google_containers/k8s-dns-kube-dns-amd64 1.14.8 50.5 MB 80cc5ea4b547 dns gcr.io/google_containers/cluster-proportional-autoscaler-amd64 1.1.2 50.5 MB 78cf3f492e6b gcr.io/google_containers/kubernetes-dashboard-amd64 v1.8.3 102 MB 0c60bcf89900 dashboard nginx 1.13 109 MB ae513a47849c 3、说明 镜像被墙并且全部镜像下载需要较多时间，建议提前下载到部署机器上。 hyperkube镜像主要用来运行k8s核心组件（例如kube-apiserver等）。 此处使用的网络组件为calico。 2. 部署集群 2.1. 下载kubespary的源码 git clone https://github.com/kubernetes-incubator/kubespray.git 2.2. 编辑配置文件 2.2.1. hosts.ini hosts.ini主要为部署节点机器信息的文件，路径为：kubespray/inventory/sample/hosts.ini。 cd kubespray # 复制一份配置进行修改 cp -rfp inventory/sample inventory/k8s vi inventory/k8s/hosts.ini 例如： hosts.ini文件可以填写部署机器的登录密码，也可以不填密码而设置ssh的免密登录。 # Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # 主机名 ssh登陆IP ssh用户名 ssh登陆密码 机器IP 子网掩码 kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 [etcd] kube-master-0 [kube-node] kube-node-41 kube-node-42 [k8s-cluster:children] kube-node kube-master [calico-rr] 2.2.2. k8s-cluster.yml k8s-cluster.yml主要为k8s集群的配置文件，路径为：kubespray/inventory/k8s/group_vars/k8s-cluster.yml。该文件可以修改安装的k8s集群的版本，参数为：kube_version: v1.9.5。具体可参考： https://github.com/kubernetes-incubator/kubespray/blob/master/inventory/sample/group_vars/k8s-cluster.yml#L22 2.3. 执行部署操作 # 进入主目录 cd kubespray # 执行部署命令 ansible-playbook -i inventory/k8s/hosts.ini cluster.yml -b -vvv -vvv 参数表示输出运行日志 如果需要重置可以执行以下命令： ansible-playbook -i inventory/k8s/hosts.ini reset.yml -b -vvv 3. 确认部署结果 3.1. ansible的部署结果 ansible命令执行完，出现以下日志，则说明部署成功，否则根据报错内容进行修改。 PLAY RECAP ***************************************************************************** kube-master-0 : ok=309 changed=30 unreachable=0 failed=0 kube-node-41 : ok=203 changed=8 unreachable=0 failed=0 kube-node-42 : ok=203 changed=8 unreachable=0 failed=0 localhost : ok=2 changed=0 unreachable=0 failed=0 以下为部分部署执行日志： kubernetes/preinstall : Update package management cache (YUM) --------------------23.96s /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/main.yml:121 kubernetes/master : Master | wait for the apiserver to be running ----------------23.44s /root/gopath/src/kubespray/roles/kubernetes/master/handlers/main.yml:79 kubernetes/preinstall : Install packages requirements ----------------------------20.20s /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/main.yml:203 kubernetes/secrets : Check certs | check if a cert already exists on node --------13.94s /root/gopath/src/kubespray/roles/kubernetes/secrets/tasks/check-certs.yml:17 gather facts from all instances --------------------------------------------------9.98s /root/gopath/src/kubespray/cluster.yml:25 kubernetes/node : install | Compare host kubelet with hyperkube container --------9.66s /root/gopath/src/kubespray/roles/kubernetes/node/tasks/install_host.yml:2 kubernetes-apps/ansible : Kubernetes Apps | Start Resources -----------------------9.27s /root/gopath/src/kubespray/roles/kubernetes-apps/ansible/tasks/main.yml:37 kubernetes-apps/ansible : Kubernetes Apps | Lay Down KubeDNS Template ------------8.47s /root/gopath/src/kubespray/roles/kubernetes-apps/ansible/tasks/kubedns.yml:3 download : Sync container ---------------------------------------------------------8.23s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 kubernetes-apps/network_plugin/calico : Start Calico resources --------------------7.82s /root/gopath/src/kubespray/roles/kubernetes-apps/network_plugin/calico/tasks/main.yml:2 download : Download items ---------------------------------------------------------7.67s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------7.48s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Sync container ---------------------------------------------------------7.35s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 download : Download items ---------------------------------------------------------7.16s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 network_plugin/calico : Calico | Copy cni plugins from calico/cni container -------7.10s /root/gopath/src/kubespray/roles/network_plugin/calico/tasks/main.yml:62 download : Download items ---------------------------------------------------------7.04s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------7.01s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Sync container ---------------------------------------------------------7.00s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 download : Download items ---------------------------------------------------------6.98s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------6.79s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 3.2. k8s集群运行结果 1、k8s组件信息 # kubectl get all --namespace=kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds/calico-node 3 3 3 3 3 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 2 2 2 2 2h deploy/kubedns-autoscaler 1 1 1 1 2h deploy/kubernetes-dashboard 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/kube-dns-79d99cdcd5 2 2 2 2h rs/kubedns-autoscaler-5564b5585f 1 1 1 2h rs/kubernetes-dashboard-69cb58d748 1 1 1 2h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds/calico-node 3 3 3 3 3 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 2 2 2 2 2h deploy/kubedns-autoscaler 1 1 1 1 2h deploy/kubernetes-dashboard 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/kube-dns-79d99cdcd5 2 2 2 2h rs/kubedns-autoscaler-5564b5585f 1 1 1 2h rs/kubernetes-dashboard-69cb58d748 1 1 1 2h NAME READY STATUS RESTARTS AGE po/calico-node-22vsg 1/1 Running 0 2h po/calico-node-t7zgw 1/1 Running 0 2h po/calico-node-zqnx8 1/1 Running 0 2h po/kube-apiserver-kube-master-0 1/1 Running 0 22h po/kube-controller-manager-kube-master-0 1/1 Running 0 2h po/kube-dns-79d99cdcd5-f2t6t 3/3 Running 0 2h po/kube-dns-79d99cdcd5-gw944 3/3 Running 0 2h po/kube-proxy-kube-master-0 1/1 Running 2 22h po/kube-proxy-kube-node-41 1/1 Running 3 22h po/kube-proxy-kube-node-42 1/1 Running 3 22h po/kube-scheduler-kube-master-0 1/1 Running 0 2h po/kubedns-autoscaler-5564b5585f-lt9bb 1/1 Running 0 2h po/kubernetes-dashboard-69cb58d748-wmb9x 1/1 Running 0 2h po/nginx-proxy-kube-node-41 1/1 Running 3 22h po/nginx-proxy-kube-node-42 1/1 Running 3 22h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.233.0.3 53/UDP,53/TCP 2h svc/kubernetes-dashboard ClusterIP 10.233.27.24 443/TCP 2h 2、k8s节点信息 # kubectl get nodes NAME STATUS ROLES AGE VERSION kube-master-0 Ready master 22h v1.9.5 kube-node-41 Ready node 22h v1.9.5 kube-node-42 Ready node 22h v1.9.5 3、组件健康信息 # kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} 4. troubles shooting 在使用kubespary部署k8s集群时，主要遇到以下报错。 4.1. python-netaddr未安装 报错内容： fatal: [node1]: FAILED! => {\"failed\": true, \"msg\": \"The ipaddr filter requires python-netaddr be installed on the ansible controller\"} 解决方法： 需要安装 python-netaddr，具体参考上述[环境准备]内容。 4.2. swap未关闭 报错内容： fatal: [kube-master-0]: FAILED! => { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-41]: FAILED! => { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-42]: FAILED! => { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } 解决方法： 所有部署机器执行swapoff -a关闭swap，具体参考上述[环境准备]内容。 4.3. 部署机器内存过小 报错内容： TASK [kubernetes/preinstall : Stop if memory is too small for masters] ********************************************************************************************************************************************************************************************************* task path: /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml:52 Friday 10 August 2018 21:50:26 +0800 (0:00:00.940) 0:01:14.088 ********* fatal: [kube-master-0]: FAILED! => { \"assertion\": \"ansible_memtotal_mb >= 1500\", \"changed\": false, \"evaluated_to\": false } TASK [kubernetes/preinstall : Stop if memory is too small for nodes] *********************************************************************************************************************************************************************************************************** task path: /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml:58 Friday 10 August 2018 21:50:27 +0800 (0:00:00.570) 0:01:14.659 ********* fatal: [kube-node-41]: FAILED! => { \"assertion\": \"ansible_memtotal_mb >= 1024\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-42]: FAILED! => { \"assertion\": \"ansible_memtotal_mb >= 1024\", \"changed\": false, \"evaluated_to\": false } to retry, use: --limit @/root/gopath/src/kubespray/cluster.retry 解决方法： 调大所有部署机器的内存，本示例中调整为3G或以上。 4.4. kube-scheduler组件运行失败 kube-scheduler组件运行失败，导致http://localhost:10251/healthz调用失败。 报错内容： FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left). FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left). fatal: [node1]: FAILED! => {\"attempts\": 60, \"changed\": false, \"content\": \"\", \"failed\": true, \"msg\": \"Status code was not [200]: Request failed: \", \"redirected\": false, \"status\": -1, \"url\": \"http://localhost:10251/healthz\"} 解决方法： 可能是内存不足导致，本示例中调大了部署机器的内存。 4.5. docker安装包冲突 报错内容： failed: [k8s-node-1] (item={u'name': u'docker-engine-1.13.1-1.el7.centos'}) => { \"attempts\": 4, \"changed\": false, ... \"item\": { \"name\": \"docker-engine-1.13.1-1.el7.centos\" }, \"msg\": \"Error: docker-ce-selinux conflicts with 2:container-selinux-2.66-1.el7.noarch\\n\", \"rc\": 1, \"results\": [ \"Loaded plugins: fastestmirror\\nLoading mirror speeds from cached hostfile\\n * elrepo: mirrors.tuna.tsinghua.edu.cn\\n * epel: mirrors.tongji.edu.cn\\nPackage docker-engine is obsoleted by docker-ce, trying to install docker-ce-17.03.2.ce-1.el7.centos.x86_64 instead\\nResolving Dependencies\\n--> Running transaction check\\n---> Package docker-ce.x86_64 0:17.03.2.ce-1.el7.centos will be installed\\n--> Processing Dependency: docker-ce-selinux >= 17.03.2.ce-1.el7.centos for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64\\n--> Processing Dependency: libltdl.so.7()(64bit) for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64\\n--> Running transaction check\\n---> Package docker-ce-selinux.noarch 0:17.03.2.ce-1.el7.centos will be installed\\n---> Package libtool-ltdl.x86_64 0:2.4.2-22.el7_3 will be installed\\n--> Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux\\n--> Restarting Dependency Resolution with new changes.\\n--> Running transaction check\\n---> Package container-selinux.noarch 2:2.55-1.el7 will be updated\\n---> Package container-selinux.noarch 2:2.66-1.el7 will be an update\\n--> Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux\\n--> Finished Dependency Resolution\\n You could try using --skip-broken to work around the problem\\n You could try running: rpm -Va --nofiles --nodigest\\n\" ] } 解决方法： 卸载旧的docker版本，由kubespary自动安装。 sudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 参考文章： https://github.com/kubernetes-incubator/kubespray Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"setup/install-k8s-by-minikube.html":{"url":"setup/install-k8s-by-minikube.html","title":"使用minikube安装kubernetes","keywords":"","body":" 以下内容基于Linux系统，特别为Ubuntu系统 1. 安装kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ 下载指定版本，例如下载v1.9.0版本 curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ 2. 安装minikube minikube的源码地址：https://github.com/kubernetes/minikube 2.1 安装minikube 以下命令为安装latest版本的minikube。 curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ 安装指定版本可到https://github.com/kubernetes/minikube/releases下载对应版本。 例如：以下为安装v0.28.2版本 curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ 2.2 minikube命令帮助 Minikube is a CLI tool that provisions and manages single-node Kubernetes clusters optimized for development workflows. Usage: minikube [command] Available Commands: addons Modify minikube's kubernetes addons cache Add or delete an image from the local cache. completion Outputs minikube shell completion for the given shell (bash or zsh) config Modify minikube config dashboard Opens/displays the kubernetes dashboard URL for your local cluster delete Deletes a local kubernetes cluster docker-env Sets up docker env variables; similar to '$(docker-machine env)' get-k8s-versions Gets the list of Kubernetes versions available for minikube when using the localkube bootstrapper ip Retrieves the IP address of the running cluster logs Gets the logs of the running localkube instance, used for debugging minikube, not user code mount Mounts the specified directory into minikube profile Profile sets the current minikube profile service Gets the kubernetes URL(s) for the specified service in your local cluster ssh Log into or run a command on a machine with SSH; similar to 'docker-machine ssh' ssh-key Retrieve the ssh identity key path of the specified cluster start Starts a local kubernetes cluster status Gets the status of a local kubernetes cluster stop Stops a running local kubernetes cluster update-check Print current and latest version number update-context Verify the IP address of the running cluster in kubeconfig. version Print the version of minikube Flags: --alsologtostderr log to standard error as well as files -b, --bootstrapper string The name of the cluster bootstrapper that will set up the kubernetes cluster. (default \"localkube\") --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --loglevel int Log level (0 = DEBUG, 5 = FATAL) (default 1) --logtostderr log to standard error instead of files -p, --profile string The name of the minikube VM being used. This can be modified to allow for multiple minikube instances to be run independently (default \"minikube\") --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Use \"minikube [command] --help\" for more information about a command. 3. 使用minikube安装k8s集群 3.1. minikube start 可以以Docker的方式运行k8s的组件，但需要先安装Docker(可参考Docker安装)，启动参数使用--vm-driver=none。 minikube start --vm-driver=none 例如： root@ubuntu:~# minikube start --vm-driver=none Starting local Kubernetes v1.10.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Downloading kubeadm v1.10.0 Downloading kubelet v1.10.0 ^[[DFinished Downloading kubelet v1.10.0 Finished Downloading kubeadm v1.10.0 Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. =================== WARNING: IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacks When using the none driver, the kubectl config and credentials generated will be root owned and will appear in the root home directory. You will need to move the files to the appropriate location and then set the correct permissions. An example of this is below: sudo mv /root/.kube $HOME/.kube # this will write over any previous configuration sudo chown -R $USER $HOME/.kube sudo chgrp -R $USER $HOME/.kube sudo mv /root/.minikube $HOME/.minikube # this will write over any previous configuration sudo chown -R $USER $HOME/.minikube sudo chgrp -R $USER $HOME/.minikube This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true Loading cached images from config file. 安装指定版本的kubernetes集群 # 查阅版本 minikube get-k8s-versions # 选择版本启动 minikube start --kubernetes-version v1.7.3 --vm-driver=none 3.2. minikube status $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 172.16.94.139 3.3. minikube stop minikube stop 命令可以用来停止集群。 该命令会关闭 minikube 虚拟机，但将保留所有集群状态和数据。 再次启动集群将恢复到之前的状态。 3.4. minikube delete minikube delete 命令可以用来删除集群。 该命令将关闭并删除 minikube 虚拟机。没有数据或状态会被保存下来。 4. 查看部署结果 4.1. 部署组件 root@ubuntu:~# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE pod/etcd-minikube 1/1 Running 0 38m pod/kube-addon-manager-minikube 1/1 Running 0 38m pod/kube-apiserver-minikube 1/1 Running 1 39m pod/kube-controller-manager-minikube 1/1 Running 0 38m pod/kube-dns-86f4d74b45-bdfnx 3/3 Running 0 38m pod/kube-proxy-dqdvg 1/1 Running 0 38m pod/kube-scheduler-minikube 1/1 Running 0 38m pod/kubernetes-dashboard-5498ccf677-c2gnh 1/1 Running 0 38m pod/storage-provisioner 1/1 Running 0 38m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.96.0.10 53/UDP,53/TCP 38m service/kubernetes-dashboard NodePort 10.104.48.227 80:30000/TCP 38m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/kube-proxy 1 1 1 1 1 38m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/kube-dns 1 1 1 1 38m deployment.apps/kubernetes-dashboard 1 1 1 1 38m NAME DESIRED CURRENT READY AGE replicaset.apps/kube-dns-86f4d74b45 1 1 1 38m replicaset.apps/kubernetes-dashboard-5498ccf677 1 1 1 38m 4.2. dashboard 通过访问ip:port，例如：http://172.16.94.139:30000/，可以访问k8s的`dashboard`控制台。 5. troubleshooting 5.1. 没有安装VirtualBox [root@minikube ~]# minikube start Starting local Kubernetes v1.10.0 cluster... Starting VM... Downloading Minikube ISO 160.27 MB / 160.27 MB [============================================] 100.00% 0s E0727 15:47:08.655647 9407 start.go:174] Error starting host: Error creating host: Error executing step: Running precreate checks. : VBoxManage not found. Make sure VirtualBox is installed and VBoxManage is in the path. Retrying. E0727 15:47:08.656994 9407 start.go:180] Error starting host: Error creating host: Error executing step: Running precreate checks. : VBoxManage not found. Make sure VirtualBox is installed and VBoxManage is in the path ================================================================================ An error has occurred. Would you like to opt in to sending anonymized crash information to minikube to help prevent future errors? To opt out of these messages, run the command: minikube config set WantReportErrorPrompt false ================================================================================ Please enter your response [Y/n]: 解决方法，先安装VirtualBox。 5.2. 没有安装Docker [root@minikube ~]# minikube start --vm-driver=none Starting local Kubernetes v1.10.0 cluster... Starting VM... E0727 15:56:54.936706 9441 start.go:174] Error starting host: Error creating host: Error executing step: Running precreate checks. : docker cannot be found on the path for this machine. A docker installation is a requirement for using the none driver: exec: \"docker\": executable file not found in $PATH. Retrying. E0727 15:56:54.938930 9441 start.go:180] Error starting host: Error creating host: Error executing step: Running precreate checks. : docker cannot be found on the path for this machine. A docker installation is a requirement for using the none driver: exec: \"docker\": executable file not found in $PATH 解决方法，先安装Docker。 文章参考： https://github.com/kubernetes/minikube https://kubernetes.io/docs/setup/minikube/ https://kubernetes.io/docs/tasks/tools/install-minikube/ https://kubernetes.io/docs/tasks/tools/install-kubectl/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"concepts/architecture/kubernetes-architecture.html":{"url":"concepts/architecture/kubernetes-architecture.html","title":"Kubernetes总架构图","keywords":"","body":"1. Kubernetes的总架构图 2. Kubernetes各个组件介绍 2.1 kube-master[控制节点] master的工作流程图 Kubecfg将特定的请求，比如创建Pod，发送给Kubernetes Client。 Kubernetes Client将请求发送给API server。 API Server根据请求的类型，比如创建Pod时storage类型是pods，然后依此选择何种REST Storage API对请求作出处理。 REST Storage API对的请求作相应的处理。 将处理的结果存入高可用键值存储系统Etcd中。 在API Server响应Kubecfg的请求后，Scheduler会根据Kubernetes Client获取集群中运行Pod及Minion/Node信息。 依据从Kubernetes Client获取的信息，Scheduler将未分发的Pod分发到可用的Minion/Node节点上。 2.1.1 API Server[资源操作入口] 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。 第一，是为了保证集群状态访问的安全。 第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。 作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。 更多API Server信息请参考：Kubernetes核心原理（一）之API Server 2.1.2 Controller Manager[内部管理控制中心] 实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 更多Controller Manager信息请参考：Kubernetes核心原理（二）之Controller Manager 2.1.3 Scheduler[集群分发调度器] Scheduler收集和分析当前Kubernetes集群中所有Minion/Node节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 Scheduler也监测Minion/Node节点信息，由于会频繁查找Minion/Node节点，Scheduler会缓存一份最新的信息在本地。 最后，Scheduler在分发Pod到指定的Minion/Node节点后，会把Pod相关的信息Binding写回API Server。 更多Scheduler信息请参考：Kubernetes核心原理（三）之Scheduler 2.2 kube-node[服务节点] kubelet结构图 2.2.1 Kubelet[节点上的Pod管家] 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理 定时上报本Node的状态信息给API Server。 kubelet是Master API Server和Minion/Node之间的桥梁，接收Master API Server分配给它的commands和work，通过kube-apiserver间接与Etcd集群交互，读取配置信息。 具体的工作如下： 1) 设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。 2) 同步Pod的状态、同步Pod的状态、从cAdvisor获取container info、 pod info、 root info、 machine info。 3) 在容器中运行命令、杀死容器、删除Pod的所有容器。 更多Kubelet信息请参考：Kubernetes核心原理（四）之Kubelet 2.2.2 Proxy[负载均衡、路由转发] Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Minion/Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion/Node上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 2.2.3 kubectl（kubelet client）[集群管理命令行工具集] 通过客户端的kubectl命令集操作，API Server响应对应的命令结果，从而达到对kubernetes集群的管理。 参考文章： https://yq.aliyun.com/articles/47308?spm=5176.100240.searchblog.19.jF7FFa Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"concepts/architecture/paas-based-on-docker-and-kubernetes.html":{"url":"concepts/architecture/paas-based-on-docker-and-kubernetes.html","title":"基于Docker及Kubernetes技术构建容器云（PaaS）平台概述","keywords":"","body":"[编者的话] 目前很多的容器云平台通过Docker及Kubernetes等技术提供应用运行平台，从而实现运维自动化，快速部署应用、弹性伸缩和动态调整应用环境资源，提高研发运营效率。 从宏观到微观（从抽象到具体）的思路来理解：云计算→PaaS→ App Engine→XAE[XXX App Engine] （XAE泛指一类应用运行平台，例如GAE、SAE、BAE等）。 本文简要介绍了与容器云相关的几个重要概念：PaaS、App Engine、Dokcer、Kubernetes。 1. PaaS概述 1.1. PaaS概念 PaaS(Platform as a service)，平台即服务，指将软件研发的平台（或业务基础平台）作为一种服务，以SaaS的模式提交给用户。 PaaS是云计算服务的其中一种模式，云计算是一种按使用量付费的模式的服务，类似一种租赁服务，服务可以是基础设施计算资源（IaaS），平台（PaaS），软件（SaaS）。租用IT资源的方式来实现业务需要，如同水力、电力资源一样，计算、存储、网络将成为企业IT运行的一种被使用的资源，无需自己建设，可按需获得。 PaaS的实质是将互联网的资源服务化为可编程接口，为第三方开发者提供有商业价值的资源和服务平台。简而言之，IaaS就是卖硬件及计算资源，PaaS就是卖开发、运行环境，SaaS就是卖软件。 1.2. IaaS/PaaS/SaaS说明 类型 说明 比喻 例子 IaaS:Infrastructure-as-a-Service(基础设施即服务) 提供的服务是计算基础设施 地皮，需要自己盖房子 Amazon EC2（亚马逊弹性云计算） PaaS: Platform-as-a-Service(平台即服务) 提供的服务是软件研发的平台或业务基础平台 商品房，需要自己装修 GAE（谷歌开发者平台） SaaS: Software-as-a-Service(软件即服务) 提供的服务是运行在云计算基础设施上的应用程序 酒店套房，可以直接入住 谷歌的Gmail邮箱 1.3. PaaS的特点（三种层次） 特点 说明 平台即服务 PaaS提供的服务就是个基础平台，一个环境，而不是具体的应用 平台及服务 不仅提供平台，还提供对该平台的技术支持、优化等服务 平台级服务 “平台级服务”即强大稳定的平台和专业的技术支持团队，保障应用的稳定使用 2. App Engine概述 2.1. App Engine概念 App Engine是PaaS模式的一种实现方式，App Engine将应用运行所需的 IT 资源和基础设施以服务的方式提供给用户，包括了中间件服务、资源管理服务、弹性调度服务、消息服务等多种服务形式。App Engine的目标是对应用提供完整生命周期（包括设计、开发、测试和部署等阶段）的支持，从而减少了用户在购置和管理应用生命周期内所必须的软硬件以及部署应用和IT 基础设施的成本，同时简化了以上工作的复杂度。常见的App Engine有：GAE(Google App Engine)，SAE(Sina App Engine)，BAE(Baidu App Engine)。 App Engine利用虚拟化与自动化技术实现快速搭建部署应用运行环境和动态调整应用运行时环境资源这两个目标。一方面实现即时部署以及快速回收，降低了环境搭建时间，避免了手工配置错误，快速重复搭建环境，及时回收资源， 减少了低利用率硬件资源的空置。另一方面，根据应用运行时的需求对应用环境进行动态调整，实现了应用平台的弹性扩展和自优化，减少了非高峰时硬件资源的空置。 简而言之，App Engine主要目标是：Easy to maintain(维护), Easy to scale(扩容), Easy to build(构建)。 2.2. 架构设计 2.3. 组成模块说明 组成模块 模块说明 App Router[流量接入层] 接收用户请求，并转发到不同的App Runtime。 App Runtime[应用运行层] 应用运行环境，为各个应用提供基本的运行引擎，从而让app能够运行起来。 Services[基础服务层] 各个通用基础服务，主要是对主流的服务提供通用的接入，例如数据库等。 Platform Control[平台控制层] 整个平台的控制中心，实现业务调度，弹性扩容、资源审计、集群管理等相关工作。 Manage System[管理界面层] 提供友好可用的管理操作界面方便平台管理员来控制管理整个平台。 Platform Support[平台支持层] 为应用提供相关的支持，比如应用监控、问题定位、分布式日志重建、统计分析等。 Log Center[日志中心] 实时收集相关应用及系统的日志（日志收集），提供实时计算和分析平台（日志处理）。 Code Center[代码中心] 完成代码存储、部署上线相关的工作。 3. 容器云平台技术栈 功能组成部分 使用工具 应用载体 Docker 编排工具 Kubernetes 配置数据 Etcd 网络管理 Flannel 存储管理 Ceph 底层实现 Linux内核的Namespace[资源隔离]和CGroups[资源控制] Namespace[资源隔离] Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于某个特定的Namespace。每个namespace下的资源对于其他namespace下的资源都是透明，不可见的。 CGroups[资源控制] CGroup（control group）是将任意进程进行分组化管理的Linux内核功能。CGroup本身是提供将进程进行分组化管理的功能和接口的基础结构，I/O或内存的分配控制等具体的资源管理功能是通过这个功能来实现的。CGroups可以限制、记录、隔离进程组所使用的物理资源（包括：CPU、memory、IO等），为容器实现虚拟化提供了基本保证。CGroups本质是内核附加在程序上的一系列钩子（hooks），通过程序运行时对资源的调度触发相应的钩子以达到资源追踪和限制的目的。 4. Docker概述 更多详情请参考：Docker整体架构图 4.1. Docker介绍 Docker - Build, Ship, and Run Any App, Anywhere Docker是一种Linux容器工具集，它是为“构建（Build）、交付（Ship）和运行（Run）”分布式应用而设计的。 Docker相当于把应用以及应用所依赖的环境完完整整地打成了一个包，这个包拿到哪里都能原生运行。因此可以在开发、测试、运维中保证环境的一致性。 Docker的本质：Docker=LXC(Namespace+CGroups)+Docker Images，即在Linux内核的Namespace[资源隔离]和CGroups[资源控制]技术的基础上通过镜像管理机制来实现轻量化设计。 4.2. Docker的基本概念 4.2.1. 镜像 Docker 镜像就是一个只读的模板，可以把镜像理解成一个模子（模具），由模子（镜像）制作的成品（容器）都是一样的（除非在生成时加额外参数），修改成品（容器）本身并不会对模子（镜像）产生影响（除非将成品提交成一个模子），容器重启时，即由模子（镜像）重新制作成一个成品（容器），与其他由该模子制作成的成品并无区别。 例如：一个镜像可以包含一个完整的 ubuntu 操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器。Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户可以直接从其他人那里下载一个已经做好的镜像来直接使用。 4.2.2. 容器 Docker 利用容器来运行应用。容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 4.2.3. 仓库 仓库是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 4.3. Docker的优势 容器的快速轻量 容器的启动，停止和销毁都是以秒或毫秒为单位的，并且相比传统的虚拟化技术，使用容器在CPU、内存，网络IO等资源上的性能损耗都有同样水平甚至更优的表现。 一次构建，到处运行 当将容器固化成镜像后，就可以非常快速地加载到任何环境中部署运行。而构建出来的镜像打包了应用运行所需的程序、依赖和运行环境， 这是一个完整可用的应用集装箱，在任何环境下都能保证环境一致性。 完整的生态链 容器技术并不是Docker首创，但是以往的容器实现只关注于如何运行，而Docker站在巨人的肩膀上进行整合和创新，特别是Docker镜像的设计，完美地解决了容器从构建、交付到运行，提供了完整的生态链支持。 5. Kubernetes概述 更多详情请参考：Kubernetes总架构图 5.1. Kubernetes介绍 Kubernetes是Google开源的容器集群管理系统。它构建Docker技术之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等整一套功能，本质上可看作是基于容器技术的Micro-PaaS平台，即第三代PaaS的代表性项目。 5.2. Kubernetes的基本概念 5.2.1. Pod Pod是若干个相关容器的组合，是一个逻辑概念，Pod包含的容器运行在同一个宿主机上，这些容器使用相同的网络命名空间、IP地址和端口，相互之间能通过localhost来发现和通信，共享一块存储卷空间。在Kubernetes中创建、调度和管理的最小单位是Pod。一个Pod一般只放一个业务容器和一个用于统一网络管理的网络容器。 5.2.2. Replication Controller Replication Controller是用来控制管理Pod副本(Replica，或者称实例)，Replication Controller确保任何时候Kubernetes集群中有指定数量的Pod副本在运行，如果少于指定数量的Pod副本，Replication Controller会启动新的Pod副本，反之会杀死多余的以保证数量不变。另外Replication Controller是弹性伸缩、滚动升级的实现核心。 5.2.3. Service Service是真实应用服务的抽象，定义了Pod的逻辑集合和访问这个Pod集合的策略，Service将代理Pod对外表现为一个单一访问接口，外部不需要了解后端Pod如何运行，这给扩展或维护带来很大的好处，提供了一套简化的服务代理和发现机制。 5.2.4. Label Label是用于区分Pod、Service、Replication Controller的Key/Value键值对，实际上Kubernetes中的任意API对象都可以通过Label进行标识。每个API对象可以有多个Label，但是每个Label的Key只能对应一个Value。Label是Service和Replication Controller运行的基础，它们都通过Label来关联Pod，相比于强绑定模型，这是一种非常好的松耦合关系。 5.2.5. Node Kubernets属于主从的分布式集群架构，Kubernets Node（简称为Node，早期版本叫做Minion）运行并管理容器。Node作为Kubernetes的操作单元，将用来分配给Pod（或者说容器）进行绑定，Pod最终运行在Node上，Node可以认为是Pod的宿主机。 5.3. Kubernetes架构 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"concepts/object/understanding-kubernetes-objects.html":{"url":"concepts/object/understanding-kubernetes-objects.html","title":"理解kubernetes对象","keywords":"","body":"1. kubernetes对象概述 kubernetes中的对象是一些持久化的实体，可以理解为是对集群状态的描述或期望。 包括： 集群中哪些node上运行了哪些容器化应用 应用的资源是否满足使用 应用的执行策略，例如重启策略、更新策略、容错策略等。 kubernetes的对象是一种意图（期望）的记录，kubernetes会始终保持预期创建的对象存在和集群运行在预期的状态下。 操作kubernetes对象（增删改查）需要通过kubernetes API，一般有以下几种方式： kubectl命令工具 Client library的方式，例如 client-go 2. Spec and Status 每个kubernetes对象的结构描述都包含spec和status两个部分。 spec：该内容由用户提供，描述用户期望的对象特征及集群状态。 status：该内容由kubernetes集群提供和更新，描述kubernetes对象的实时状态。 任何时候，kubernetes都会控制集群的实时状态status与用户的预期状态spec一致。 例如：当你定义Deployment的描述文件，指定集群中运行3个实例，那么kubernetes会始终保持集群中运行3个实例，如果任何实例挂掉，kubernetes会自动重建新的实例来保持集群中始终运行用户预期的3个实例。 3. 对象描述文件 当你要创建一个kubernetes对象的时候，需要提供该对象的描述信息spec，来描述你的对象在kubernetes中的预期状态。 一般使用kubernetes API来创建kubernetes对象，其中spec信息可以以JSON的形式存放在request body中，也可以以.yaml文件的形式通过kubectl工具创建。 例如，以下为Deployment对象对应的yaml文件： apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 执行kubectl create的命令 #create command kubectl create -f https://k8s.io/docs/user-guide/nginx-deployment.yaml --record #output deployment \"nginx-deployment\" created 4. 必须字段 在对象描述文件.yaml中，必须包含以下字段。 apiVersion：kubernetes API的版本 kind：kubernetes对象的类型 metadata：唯一标识该对象的元数据，包括name，UID，可选的namespace spec：标识对象的详细信息，不同对象的spec的格式不同，可以嵌套其他对象的字段。 文章参考： https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"concepts/object/kubernetes-basic-concepts.html":{"url":"concepts/object/kubernetes-basic-concepts.html","title":"kubernetes常用对象说明","keywords":"","body":"1. Master 集群的控制节点，负责整个集群的管理和控制，kubernetes的所有的命令基本都是发给Master，由它来负责具体的执行过程。 1.1. Master的组件 kube-apiserver：资源增删改查的入口 kube-controller-manager：资源对象的大总管 kube-scheduler：负责资源调度（Pod调度） etcd Server:kubernetes的所有的资源对象的数据保存在etcd中。 2. Node Node是集群的工作负载节点，默认情况kubelet会向Master注册自己，一旦Node被纳入集群管理范围，kubelet会定时向Master汇报自身的情报，包括操作系统，Docker版本，机器资源情况等。 如果Node超过指定时间不上报信息，会被Master判断为“失联”，标记为Not Ready，随后Master会触发Pod转移。 2.1. Node的组件 kubelet:Pod的管家，与Master通信 kube-proxy：实现kubernetes Service的通信与负载均衡机制的重要组件 Docker：容器的创建和管理 2.2. Node相关命令 kubectl get nodes kuebctl describe node {node_name} 2.3. describe命令的Node信息 Node基本信息：名称、标签、创建时间等 Node当前的状态，Node启动后会进行自检工作，磁盘是否满，内存是否不足，若都正常则切换为Ready状态。 Node的主机地址与主机名 Node上的资源总量：CPU,内存，最大可调度Pod数量等 Node可分配资源量：当前Node可用于分配的资源量 主机系统信息：主机唯一标识符UUID，Linux kernel版本号，操作系统，kubernetes版本，kubelet与kube-proxy版本 当前正在运行的Pod列表及概要信息 已分配的资源使用概要，例如资源申请的最低、最大允许使用量占系统总量的百分比 Node相关的Event信息。 3. Pod Pod是Kubernetes中操作的基本单元。每个Pod中有个根容器(Pause容器)，Pause容器的状态代表整个容器组的状态，其他业务容器共享Pause的IP，即Pod IP，共享Pause挂载的Volume，这样简化了同个Pod中不同容器之间的网络问题和文件共享问题。 图片 - pod Kubernetes集群中，同宿主机的或不同宿主机的Pod之间要求能够TCP/IP直接通信，因此采用虚拟二层网络技术来实现，例如Flannel，Openvswitch(OVS)等，这样在同个集群中，不同的宿主机的Pod IP为不同IP段的IP，集群中的所有Pod IP都是唯一的，不同Pod之间可以直接通信。 Pod有两种类型：普通Pod和静态Pod。静态Pod即不通过K8S调度和创建，直接在某个具体的Node机器上通过具体的文件来启动。普通Pod则是由K8S创建、调度，同时数据存放在ETCD中。 Pod IP和具体的容器端口（ContainnerPort）组成一个具体的通信地址，即Endpoint。一个Pod中可以存在多个容器，可以有多个端口，Pod IP一样，即有多个Endpoint。 Pod Volume是定义在Pod之上，被各个容器挂载到自己的文件系统中，可以用分布式文件系统实现后端存储功能。 Pod中的Event事件可以用来排查问题，可以通过kubectl describe pod xxx 来查看对应的事件。 每个Pod可以对其能使用的服务器上的计算资源设置限额，一般为CPU和Memory。K8S中一般将千分之一个的CPU配置作为最小单位，用m表示，是一个绝对值，即100m对于一个Core的机器还是48个Core的机器都是一样的大小。Memory配额也是个绝对值，单位为内存字节数。 资源配额的两个参数 Requests:该资源的最小申请量，系统必须满足要求。 Limits:该资源最大允许使用量，当超过该量，K8S会kill并重启Pod。 图片 - pod2 4. Label Label是一个键值对，可以附加在任何对象上，比如Node,Pod,Service,RC等。Label和资源对象是多对多的关系，即一个Label可以被添加到多个对象上，一个对象也可以定义多个Label。 Label的作用主要用来实现精细的、多维度的资源分组管理，以便进行资源分配，调度，配置，部署等工作。 Label通俗理解就是“标签”，通过标签来过滤筛选指定的对象，进行具体的操作。k8s通过Label Selector(标签选择器)来筛选指定Label的资源对象，类似SQL语句中的条件查询（WHERE语句）。 Label Selector有基于等式和基于集合的两种表达方式，可以多个条件进行组合使用。 基于等式：name=redis-slave（匹配name=redis-slave的资源对象）;env!=product(匹配所有不具有标签env=product的资源对象) 基于集合：name in (redis-slave,redis-master);name not in (php-frontend)（匹配所有不具有标签name=php-frontend的资源对象） 使用场景 kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本数，从而实现副本数始终保持预期数目。 kube-proxy进程通过Service的Label Selector来选择对应Pod，自动建立每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制。 kube-scheduler实现Pod定向调度：对Node定义特定的Label，并且在Pod定义文件中使用NodeSelector标签调度策略。 5. Replication Controller(RC) RC是k8s系统中的核心概念，定义了一个期望的场景。 主要包括： Pod期望的副本数（replicas） 用于筛选目标Pod的Label Selector 用于创建Pod的模板（template） RC特性说明： Pod的缩放可以通过以下命令实现：kubectl scale rc redis-slave --replicas=3 删除RC并不会删除该RC创建的Pod，可以将副本数设置为0，即可删除对应Pod。或者通过kubectl stop /delete命令来一次性删除RC和其创建的Pod。 改变RC中Pod模板的镜像版本可以实现滚动升级（Rolling Update）。具体操作见https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/ Kubernetes1.2以上版本将RC升级为Replica Set，它与当前RC的唯一区别在于Replica Set支持基于集合的Label Selector(Set-based selector)，而旧版本RC只支持基于等式的Label Selector(equality-based selector)。 Kubernetes1.2以上版本通过Deployment来维护Replica Set而不是单独使用Replica Set。即控制流为：Delpoyment→Replica Set→Pod。即新版本的Deployment+Replica Set替代了RC的作用。 6. Deployment Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。 使用场景 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。 检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。 更新Deployment以创建新的Pod(例如镜像升级的场景)。 如果当前Deployment不稳定，回退到上一个Deployment版本。 挂起或恢复一个Deployment。 可以通过kubectl describe deployment来查看Deployment控制的Pod的水平拓展过程。 7. Horizontal Pod Autoscaler(HPA) Horizontal Pod Autoscaler(HPA)即Pod横向自动扩容，与RC一样也属于k8s的资源对象。 HPA原理：通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否针对性调整Pod的副本数。 Pod负载度量指标： CPUUtilizationPercentage：Pod所有副本自身的CPU利用率的平均值。即当前Pod的CPU使用量除以Pod Request的值。 应用自定义的度量指标，比如服务每秒内响应的请求数（TPS/QPS）。 8. Service(服务) 8.1. Service概述 图片 - service Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。 8.2. kubernetes的服务发现机制 主要通过kube-dns这个组件来进行DNS方式的服务发现。 8.3. 外部系统访问Service的问题 IP类型 说明 Node IP Node节点的IP地址 Pod IP Pod的IP地址 Cluster IP Service的IP地址 8.3.1. Node IP NodeIP是集群中每个节点的物理网卡IP地址，是真实存在的物理网络，kubernetes集群之外的节点访问kubernetes内的某个节点或TCP/IP服务的时候，需要通过NodeIP进行通信。 8.3.2. Pod IP Pod IP是每个Pod的IP地址，是Docker Engine根据docker0网桥的IP段地址进行分配的，是一个虚拟二层网络，集群中一个Pod的容器访问另一个Pod中的容器，是通过Pod IP进行通信的，而真实的TCP/IP流量是通过Node IP所在的网卡流出的。 8.3.3. Cluster IP Service的Cluster IP是一个虚拟IP，只作用于Service这个对象，由kubernetes管理和分配IP地址（来源于Cluster IP地址池）。 Cluster IP无法被ping通，因为没有一个实体网络对象来响应。 Cluster IP结合Service Port组成的具体通信端口才具备TCP/IP通信基础，属于kubernetes集群内，集群外访问该IP和端口需要额外处理。 k8s集群内Node IP 、Pod IP、Cluster IP之间的通信采取k8s自己的特殊的路由规则，与传统IP路由不同。 8.3.4. 外部访问Kubernetes集群 通过宿主机与容器端口映射的方式进行访问，例如：Service定位文件如下： 可以通过任意Node的IP 加端口访问该服务。也可以通过Nginx或HAProxy来设置负载均衡。 9. Volume(存储卷) 9.1. Volume的功能 Volume是Pod中能够被多个容器访问的共享目录，可以让容器的数据写到宿主机上或者写文件到网络存储中 可以实现容器配置文件集中化定义与管理，通过ConfigMap资源对象来实现。 9.2. Volume的特点 k8s中的Volume与Docker的Volume相似，但不完全相同。 k8s上Volume定义在Pod上，然后被一个Pod中的多个容器挂载到具体的文件目录下。 k8s的Volume与Pod生命周期相关而不是容器是生命周期，即容器挂掉，数据不会丢失但是Pod挂掉，数据则会丢失。 k8s中的Volume支持多种类型的Volume：Ceph、GlusterFS等分布式系统。 9.3. Volume的使用方式 先在Pod上声明一个Volume，然后容器引用该Volume并Mount到容器的某个目录。 9.4. Volume类型 9.4.1. emptyDir emptyDir Volume是在Pod分配到Node时创建的，初始内容为空，无须指定宿主机上对应的目录文件，由K8S自动分配一个目录，当Pod被删除时，对应的emptyDir数据也会永久删除。 作用： 临时空间，例如程序的临时文件，无须永久保留 长时间任务的中间过程CheckPoint的临时保存目录 一个容器需要从另一个容器中获取数据的目录（即多容器共享目录） 说明： 目前用户无法设置emptyVolume的使用介质，如果kubelet的配置使用硬盘则emptyDir将创建在该硬盘上。 9.4.2. hostPath hostPath是在Pod上挂载宿主机上的文件或目录。 作用： 容器应用日志需要持久化时，可以使用宿主机的高速文件系统进行存储 需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统。 注意点： 在不同的Node上具有相同配置的Pod可能会因为宿主机上的目录或文件不同导致对Volume上目录或文件的访问结果不一致。 如果使用了资源配额管理，则kubernetes无法将hostPath在宿主机上使用的资源纳入管理。 9.4.3. gcePersistentDisk 表示使用谷歌公有云提供的永久磁盘（Persistent Disk ,PD）存放Volume的数据，它与EmptyDir不同，PD上的内容会被永久保存。当Pod被删除时，PD只是被卸载时，但不会被删除。需要先创建一个永久磁盘，才能使用gcePersistentDisk。 使用gcePersistentDisk的限制条件： Node(运行kubelet的节点)需要是GCE虚拟机。 虚拟机需要与PD存在于相同的GCE项目中和Zone中。 10. Persistent Volume Volume定义在Pod上，属于“计算资源”的一部分，而Persistent Volume和Persistent Volume Claim是网络存储，简称PV和PVC，可以理解为k8s集群中某个网络存储中对应的一块存储。 PV是网络存储，不属于任何Node，但可以在每个Node上访问。 PV不是定义在Pod上，而是独立于Pod之外定义。 PV常见类型：GCE Persistent Disks、NFS、RBD等。 PV是有状态的对象，状态类型如下： Available:空闲状态 Bound:已经绑定到某个PVC上 Released:对应的PVC已经删除，但资源还没有回收 Failed:PV自动回收失败 11. Namespace Namespace即命名空间，主要用于多租户的资源隔离，通过将资源对象分配到不同的Namespace上，便于不同的分组在共享资源的同时可以被分别管理。 k8s集群启动后会默认创建一个“default”的Namespace。可以通过kubectl get namespaecs查看。 可以通过kubectl config use-context namespace配置当前k8s客户端的环境，通过kubectl get pods获取当前namespace的Pod。或者通过kubectl get pods --namespace=NAMESPACE来获取指定namespace的Pod。 Namespace yaml文件的定义 12. Annotation(注解) Annotation与Label类似，也使用key/value的形式进行定义，Label定义元数据（Metadata）,Annotation定义“附加”信息。 通常Annotation记录信息如下： build信息，release信息，Docker镜像信息等。 日志库、监控库等。 参考《Kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"concepts/object/kubernetes-pod-introduction.html":{"url":"concepts/object/kubernetes-pod-introduction.html","title":"Pod详解","keywords":"","body":"1. Pod的基本用法 1.1. 说明 Pod实际上是容器的集合，在k8s中对运行容器的要求为：容器的主程序需要一直在前台运行，而不是后台运行。应用可以改造成前台运行的方式，例如Go语言的程序，直接运行二进制文件；java语言则运行主类；tomcat程序可以写个运行脚本。或者通过supervisor的进程管理工具，即supervisor在前台运行，应用程序由supervisor管理在后台运行。具体可参考supervisord。 当多个应用之间是紧耦合的关系时，可以将多个应用一起放在一个Pod中，同个Pod中的多个容器之间互相访问可以通过localhost来通信（可以把Pod理解成一个虚拟机，共享网络和存储卷）。 1.2. Pod相关命令 操作 命令 说明 创建 kubectl create -f frontend-localredis-pod.yaml 查询Pod运行状态 kubectl get pods --namespace= 查询Pod详情 kebectl describe pod --namespace= 该命令常用来排查问题，查看Event事件 删除 kubectl delete pod ;kubectl delete pod --all 更新 kubectl replace pod.yaml 2. Pod的定义文件 apiVersion: v1 kind: Pod metadata: name: string namaspace: string labels: - name: string annotations: - name: string spec: containers: - name: string images: string imagePullPolice: [Always | Never | IfNotPresent] command: [string] args: [string] workingDir: string volumeMounts: - name: string mountPath: string readOnly: boolean ports: - name: string containerPort: int hostPort: int protocol: string env: - name: string value: string resources: limits: cpu: string memory: string requests: cpu: string memory: string livenessProbe: exec: command: [string] httpGet: path: string port: int host: string scheme: string httpHeaders: - name: string value: string tcpSocket: port: int initialDelaySeconds: number timeoutSeconds: number periodSeconds: number successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] nodeSelector: object imagePullSecrets: - name: string hostNetwork: false volumes: - name: string emptyDir: {} hostPath: path: string secret: secretName: string items: - key: string path: string configMap: name: string items: - key: string path: string 3. 静态pod 静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod。它们不能通过API Server进行管理，无法与ReplicationController、Deployment或DaemonSet进行关联，并且kubelet也无法对其健康检查。 静态Pod总是由kubelet创建，并且总在kubelet所在的Node上运行。 创建静态Pod的方式： 3.1. 通过配置文件方式 需要设置kubelet的启动参数“–config”，指定kubelet需要监控的配置文件所在目录，kubelet会定期扫描该目录，并根据该目录的.yaml或.json文件进行创建操作。静态Pod无法通过API Server删除（若删除会变成pending状态），如需删除该Pod则将yaml或json文件从这个目录中删除。 例如： 配置目录为/etc/kubelet.d/，配置启动参数：--config=/etc/kubelet.d/，该目录下放入static-web.yaml。 apiVersion: v1 kind: Pod metadata: name: static-web labels: name: static-web spec: containers: - name: static-web image: nginx ports: - name: web containerPort: 80 4. Pod容器共享Volume 同一个Pod中的多个容器可以共享Pod级别的存储卷Volume,Volume可以定义为各种类型，多个容器各自进行挂载，将Pod的Volume挂载为容器内部需要的目录。 例如：Pod级别的Volume:\"app-logs\",用于tomcat向其中写日志文件，busybox读日志文件。 图片 - 这里写图片描述 pod-volumes-applogs.yaml apiVersion: v1 kind: Pod metadata: name: volume-pod spec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080 volumeMounts: - name: app-logs mountPath: /usr/local/tomcat/logs - name: busybox image: busybox command: [\"sh\",\"-c\",\"tailf /logs/catalina*.log\"] volumeMounts: - name: app-logs mountPath: /logs volumes: - name: app-logs emptuDir: {} 查看日志 kubectl logs -c kubectl exec -it -c – tail /usr/local/tomcat/logs/catalina.xx.log 5. Pod的配置管理 Kubernetes v1.2的版本提供统一的集群配置管理方案–ConfigMap。 5.1. ConfigMap：容器应用的配置管理 使用场景： 生成为容器内的环境变量。 设置容器启动命令的启动参数（需设置为环境变量）。 以Volume的形式挂载为容器内部的文件或目录。 ConfigMap以一个或多个key:value的形式保存在kubernetes系统中供应用使用，既可以表示一个变量的值（例如：apploglevel=info），也可以表示完整配置文件的内容（例如：server.xml=...）。 可以通过yaml配置文件或者使用kubectl create configmap命令的方式创建ConfigMap。 5.2. 创建ConfigMap 5.2.1. 通过yaml文件方式 cm-appvars.yaml apiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: info appdatadir: /var/data 常用命令 kubectl create -f cm-appvars.yaml kubectl get configmap kubectl describe configmap cm-appvars kubectl get configmap cm-appvars -o yaml 5.2.2. 通过kubectl命令行方式 通过kubectl create configmap创建，使用参数--from-file或--from-literal指定内容，可以在一行中指定多个参数。 1）通过--from-file参数从文件中进行创建，可以指定key的名称，也可以在一个命令行中创建包含多个key的ConfigMap。 kubectl create configmap NAME --from-file=[key=]source --from-file=[key=]source 2）通过--from-file参数从目录中进行创建，该目录下的每个配置文件名被设置为key，文件内容被设置为value。 kubectl create configmap NAME --from-file=config-files-dir 3）通过--from-literal从文本中进行创建，直接将指定的key=value创建为ConfigMap的内容。 kubectl create configmap NAME --from-literal=key1=value1 --from-literal=key2=value2 容器应用对ConfigMap的使用有两种方法： 通过环境变量获取ConfigMap中的内容。 通过Volume挂载的方式将ConfigMap中的内容挂载为容器内部的文件或目录。 5.2.3. 通过环境变量的方式 ConfigMap的yaml文件:cm-appvars.yaml apiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: info appdatadir: /var/data Pod的yaml文件：cm-test-pod.yaml apiVersion: v1 kind: Pod metadata: name: cm-test-pod spec: containers: - name: cm-test image: busybox command: [\"/bin/sh\",\"-c\",\"env|grep APP\"] env: - name: APPLOGLEVEL valueFrom: configMapKeyRef: name: cm-appvars key: apploglevel - name: APPDATADIR valueFrom: configMapKeyRef: name: cm-appvars key: appdatadir 创建命令： kubectl create -f cm-test-pod.yaml kubectl get pods --show-all kubectl logs cm-test-pod 5.3. 使用ConfigMap的限制条件 ConfigMap必须在Pod之前创建 ConfigMap也可以定义为属于某个Namespace。只有处于相同Namespace中的Pod可以引用它。 kubelet只支持可以被API Server管理的Pod使用ConfigMap。静态Pod无法引用。 在Pod对ConfigMap进行挂载操作时，容器内只能挂载为“目录”，无法挂载为文件。 6. Pod的生命周期 6.1. Pod的状态 状态值 说明 Pending API Server已经创建了该Pod,但Pod中的一个或多个容器的镜像还没有创建，包括镜像下载过程 Running Pod内所有容器已创建，且至少一个容器处于运行状态、正在启动状态或正在重启状态 Succeeded Pod内所有容器均成功执行退出，且不会再重启 Failed Pod内所有容器均已退出，但至少一个容器退出失败 Unknown 由于某种原因无法获取Pod状态，例如网络通信不畅 6.2. Pod的重启策略 重启策略 说明 Always 当容器失效时，由kubelet自动重启该容器 OnFailure 当容器终止运行且退出码不为0时，由kubelet自动重启该容器 Never 不论容器运行状态如何，kubelet都不会重启该容器 说明： 可以管理Pod的控制器有Replication Controller，Job，DaemonSet，及kubelet（静态Pod）。 RC和DaemonSet：必须设置为Always，需要保证该容器持续运行。 Job：OnFailure或Never，确保容器执行完后不再重启。 kubelet：在Pod失效的时候重启它，不论RestartPolicy设置为什么值，并且不会对Pod进行健康检查。 6.3. 常见的状态转换场景 Pod的容器数 Pod当前状态 发生的事件 Pod结果状态 RestartPolicy=Always RestartPolicy=OnFailure RestartPolicy=Never 包含一个容器 Running 容器成功退出 Running Succeeded Succeeded 包含一个容器 Running 容器失败退出 Running Running Failure 包含两个容器 Running 1个容器失败退出 Running Running Running 包含两个容器 Running 容器被OOM杀掉 Running Running Failure 7. Pod健康检查 Pod的健康状态由两类探针来检查：LivenessProbe和ReadinessProbe。 LivenessProbe 用于判断容器是否存活（running状态）。 如果LivenessProbe探针探测到容器非健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。 如果容器不包含LivenessProbe探针，则kubelet认为该探针的返回值永远为“success”。 ReadinessProbe 用于判断容器是否启动完成（read状态），可以接受请求。 如果ReadnessProbe探针检测失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Endpoint。 kubelet定期执行LivenessProbe探针来判断容器的健康状态。 LivenessProbe参数： initialDelaySeconds：启动容器后首次进行健康检查的等待时间，单位为秒。 timeoutSeconds:健康检查发送请求后等待响应的时间，如果超时响应kubelet则认为容器非健康，重启该容器，单位为秒。 LivenessProbe三种实现方式： 1）ExecAction:在一个容器内部执行一个命令，如果该命令状态返回值为0，则表明容器健康。 apiVersion: v1 kind: Pod metadata: name: liveness-exec spec: containers: - name: liveness image: tomcagcr.io/google_containers/busybox args: - /bin/sh - -c - echo ok > /tmp/health;sleep 10;rm -fr /tmp/health;sleep 600 livenessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 15 timeoutSeconds: 1 2）TCPSocketAction:通过容器IP地址和端口号执行TCP检查，如果能够建立TCP连接，则表明容器健康。 apiVersion: v1 kind: Pod metadata: name: pod-with-healthcheck spec: containers: - name: nginx image: nginx ports: - containnerPort: 80 livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 15 timeoutSeconds: 1 3）HTTPGetAction:通过容器的IP地址、端口号及路径调用HTTP Get方法，如果响应的状态码大于等于200且小于等于400，则认为容器健康。 apiVersion: v1 kind: Pod metadata: name: pod-with-healthcheck spec: containers: - name: nginx image: nginx ports: - containnerPort: 80 livenessProbe: httpGet: path: /_status/healthz port: 80 initialDelaySeconds: 15 timeoutSeconds: 1 8. Pod调度 在kubernetes集群中，Pod（container）是应用的载体，一般通过RC、Deployment、DaemonSet、Job等对象来完成Pod的调度与自愈功能。 8.1. RC、Deployment:全自动调度 RC的功能即保持集群中始终运行着指定个数的Pod。 在调度策略上主要有： 系统内置调度算法[最优Node] NodeSelector[定向调度] NodeAffinity[亲和性调度] 8.1.1. NodeSelector[定向调度] k8s中kube-scheduler负责实现Pod的调度，内部系统通过一系列算法最终计算出最佳的目标节点。如果需要将Pod调度到指定Node上，则可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配来达到目的。 1、kubectl label nodes {node-name} {label-key}={label-value} 2、nodeSelector: {label-key}:{label-value} 如果给多个Node打了相同的标签，则scheduler会根据调度算法从这组Node中选择一个可用的Node来调度。 如果Pod的nodeSelector的标签在Node中没有对应的标签，则该Pod无法被调度成功。 Node标签的使用场景： 对集群中不同类型的Node打上不同的标签，可控制应用运行Node的范围。例如role=frontend;role=backend;role=database。 8.1.2. NodeAffinity[亲和性调度] NodeAffinity意为Node亲和性调度策略，NodeSelector为精确匹配，NodeAffinity为条件范围匹配，通过In（属于）、NotIn（不属于）、Exists（存在一个条件）、DoesNotExist（不存在）、Gt（大于）、Lt（小于）等操作符来选择Node，使调度更加灵活。 RequiredDuringSchedulingRequiredDuringExecution：类似于NodeSelector，但在Node不满足条件时，系统将从该Node上移除之前调度上的Pod。 RequiredDuringSchedulingIgnoredDuringExecution：与上一个类似，区别是在Node不满足条件时，系统不一定从该Node上移除之前调度上的Pod。 PreferredDuringSchedulingIgnoredDuringExecution：指定在满足调度条件的Node中，哪些Node应更优先地进行调度。同时在Node不满足条件时，系统不一定从该Node上移除之前调度上的Pod。 如果同时设置了NodeSelector和NodeAffinity，则系统将需要同时满足两者的设置才能进行调度。 8.1.3. DaemonSet：特定场景调度 DaemonSet是kubernetes1.2版本新增的一种资源对象，用于管理在集群中每个Node上仅运行一份Pod的副本实例。 图片 - 这里写图片描述 该用法适用的应用场景： 在每个Node上运行一个GlusterFS存储或者Ceph存储的daemon进程。 在每个Node上运行一个日志采集程序：fluentd或logstach。 在每个Node上运行一个健康程序，采集该Node的运行性能数据，例如：Prometheus Node Exportor、collectd、New Relic agent或Ganglia gmond等。 DaemonSet的Pod调度策略与RC类似，除了使用系统内置算法在每台Node上进行调度，也可以通过NodeSelector或NodeAffinity来指定满足条件的Node范围进行调度。 8.1.4. Job：批处理调度 kubernetes从1.2版本开始支持批处理类型的应用，可以通过kubernetes Job资源对象来定义并启动一个批处理任务。批处理任务通常并行（或串行）启动多个计算进程去处理一批工作项（work item），处理完后，整个批处理任务结束。 8.1.4.1. 批处理的三种模式 图片 - 这里写图片描述 批处理按任务实现方式不同分为以下几种模式： Job Template Expansion模式 一个Job对象对应一个待处理的Work item，有几个Work item就产生几个独立的Job，通过适用于Work item数量少，每个Work item要处理的数据量比较大的场景。例如有10个文件（Work item）,每个文件（Work item）为100G。 Queue with Pod Per Work Item 采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，其中Job会启动N个Pod，每个Pod对应一个Work item。 Queue with Variable Pod Count 采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，其中Job会启动N个Pod，每个Pod对应一个Work item。但Pod的数量是可变的。 8.1.4.2. Job的三种类型 1）Non-parallel Jobs 通常一个Job只启动一个Pod,除非Pod异常才会重启该Pod,一旦此Pod正常结束，Job将结束。 2）Parallel Jobs with a fixed completion count 并行Job会启动多个Pod，此时需要设定Job的.spec.completions参数为一个正数，当正常结束的Pod数量达到该值则Job结束。 3）Parallel Jobs with a work queue 任务队列方式的并行Job需要一个独立的Queue，Work item都在一个Queue中存放，不能设置Job的.spec.completions参数。 此时Job的特性： 每个Pod能独立判断和决定是否还有任务项需要处理 如果某个Pod正常结束，则Job不会再启动新的Pod 如果一个Pod成功结束，则此时应该不存在其他Pod还在干活的情况，它们应该都处于即将结束、退出的状态 如果所有的Pod都结束了，且至少一个Pod成功结束，则整个Job算是成功结束 9. Pod伸缩 k8s中RC的用来保持集群中始终运行指定数目的实例，通过RC的scale机制可以完成Pod的扩容和缩容（伸缩）。 9.1. 手动伸缩（scale） kubectl scale rc redis-slave --replicas=3 9.2. 自动伸缩（HPA） Horizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。HPA控制器基于Master的kube-controller-manager服务启动参数--horizontal-pod-autoscaler-sync-period定义是时长（默认30秒），周期性监控目标Pod的CPU使用率，并在满足条件时对ReplicationController或Deployment中的Pod副本数进行调整，以符合用户定义的平均Pod CPU使用率。Pod CPU使用率来源于heapster组件，因此需安装该组件。 可以通过kubectl autoscale命令进行快速创建或者使用yaml配置文件进行创建。创建之前需已存在一个RC或Deployment对象，并且该RC或Deployment中的Pod必须定义resources.requests.cpu的资源请求值，以便heapster采集到该Pod的CPU。 9.2.1. 通过kubectl autoscale创建 例如： php-apache-rc.yaml apiVersion: v1 kind: ReplicationController metadata: name: php-apache spec: replicas: 1 template: metadata: name: php-apache labels: app: php-apache spec: containers: - name: php-apache image: gcr.io/google_containers/hpa-example resources: requests: cpu: 200m ports: - containerPort: 80 创建php-apache的RC kubectl create -f php-apache-rc.yaml php-apache-svc.yaml apiVersion: v1 kind: Service metadata: name: php-apache spec: ports: - port: 80 selector: app: php-apache 创建php-apache的Service kubectl create -f php-apache-svc.yaml 创建HPA控制器 kubectl autoscale rc php-apache --min=1 --max=10 --cpu-percent=50 9.2.2. 通过yaml配置文件创建 hpa-php-apache.yaml apiVersion: v1 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: v1 kind: ReplicationController name: php-apache minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 创建hpa kubectl create -f hpa-php-apache.yaml 查看hpa kubectl get hpa 10. Pod滚动升级 k8s中的滚动升级通过执行kubectl rolling-update命令完成，该命令创建一个新的RC（与旧的RC在同一个命名空间中），然后自动控制旧的RC中的Pod副本数逐渐减少为0，同时新的RC中的Pod副本数从0逐渐增加到附加值，但滚动升级中Pod副本数（包括新Pod和旧Pod）保持原预期值。 10.1. 通过配置文件实现 redis-master-controller-v2.yaml apiVersion: v1 kind: ReplicationController metadata: name: redis-master-v2 labels: name: redis-master version: v2 spec: replicas: 1 selector: name: redis-master version: v2 template: metadata: labels: name: redis-master version: v2 spec: containers: - name: master image: kubeguide/redis-master:2.0 ports: - containerPort: 6379 注意事项： RC的名字（name）不能与旧RC的名字相同 在selector中应至少有一个Label与旧的RC的Label不同，以标识其为新的RC。例如本例中新增了version的Label。 运行kubectl rolling-update kubectl rolling-update redis-master -f redis-master-controller-v2.yaml 10.2. 通过kubectl rolling-update命令实现 kubectl rolling-update redis-master --image=redis-master:2.0 与使用配置文件实现不同在于，该执行结果旧的RC被删除，新的RC仍使用旧的RC的名字。 10.3. 升级回滚 kubectl rolling-update加参数--rollback实现回滚操作 kubectl rolling-update redis-master --image=kubeguide/redis-master:2.0 --rollback 参考《Kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"principle/kubernetes-core-principle-api-server.html":{"url":"principle/kubernetes-core-principle-api-server.html","title":"Api Server","keywords":"","body":"1. API Server简介 k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。 kubernetes API Server的功能： 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; 是资源配额控制的入口； 拥有完备的集群安全机制. kube-apiserver工作原理图 图片 - kube-apiserver 2. 如何访问kubernetes API k8s通过kube-apiserver这个进程提供服务，该进程运行在单个k8s-master节点上。默认有两个端口。 2.1. 本地端口 该端口用于接收HTTP请求； 该端口默认值为8080，可以通过API Server的启动参数“--insecure-port”的值来修改默认值； 默认的IP地址为“localhost”，可以通过启动参数“--insecure-bind-address”的值来修改该IP地址； 非认证或授权的HTTP请求通过该端口访问API Server。 2.2. 安全端口 该端口默认值为6443，可通过启动参数“--secure-port”的值来修改默认值； 默认IP地址为非本地（Non-Localhost）网络端口，通过启动参数“--bind-address”设置该值； 该端口用于接收HTTPS请求； 用于基于Tocken文件或客户端证书及HTTP Base的认证； 用于基于策略的授权； 默认不启动HTTPS安全访问控制。 2.3. 访问方式 Kubernetes REST API可参考https://kubernetes.io/docs/api-reference/v1.6/ 2.3.1. curl curl localhost:8080/api curl localhost:8080/api/v1/pods curl localhost:8080/api/v1/services curl localhost:8080/api/v1/replicationcontrollers 2.3.2. Kubectl Proxy Kubectl Proxy代理程序既能作为API Server的反向代理，也能作为普通客户端访问API Server的代理。通过master节点的8080端口来启动该代理程序。 kubectl proxy --port=8080 & 具体见kubectl proxy --help [root@node5 ~]# kubectl proxy --help To proxy all of the kubernetes api and nothing else, use: kubectl proxy --api-prefix=/ To proxy only part of the kubernetes api and also some static files: kubectl proxy --www=/my/files --www-prefix=/static/ --api-prefix=/api/ The above lets you 'curl localhost:8001/api/v1/pods'. To proxy the entire kubernetes api at a different root, use: kubectl proxy --api-prefix=/custom/ The above lets you 'curl localhost:8001/custom/api/v1/pods' Usage: kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags] Examples: # Run a proxy to kubernetes apiserver on port 8011, serving static content from ./local/www/ $ kubectl proxy --port=8011 --www=./local/www/ # Run a proxy to kubernetes apiserver on an arbitrary local port. # The chosen port for the server will be output to stdout. $ kubectl proxy --port=0 # Run a proxy to kubernetes apiserver, changing the api prefix to k8s-api # This makes e.g. the pods api available at localhost:8011/k8s-api/v1/pods/ $ kubectl proxy --api-prefix=/k8s-api Flags: --accept-hosts=\"^localhost$,^127//.0//.0//.1$,^//[::1//]$\": Regular expression for hosts that the proxy should accept. --accept-paths=\"^/.*\": Regular expression for paths that the proxy should accept. --api-prefix=\"/\": Prefix to serve the proxied API under. --disable-filter[=false]: If true, disable request filtering in the proxy. This is dangerous, and can leave you vulnerable to XSRF attacks, when used with an accessible port. -p, --port=8001: The port on which to run the proxy. Set to 0 to pick a random port. --reject-methods=\"POST,PUT,PATCH\": Regular expression for HTTP methods that the proxy should reject. --reject-paths=\"^/api/.*/exec,^/api/.*/run\": Regular expression for paths that the proxy should reject. -u, --unix-socket=\"\": Unix socket on which to run the proxy. -w, --www=\"\": Also serve static files from the given directory under the specified prefix. -P, --www-prefix=\"/static/\": Prefix to serve static files under, if static file directory is specified. Global Flags: --alsologtostderr[=false]: log to standard error as well as files --api-version=\"\": The API version to use when talking to the server --certificate-authority=\"\": Path to a cert. file for the certificate authority. --client-certificate=\"\": Path to a client key file for TLS. --client-key=\"\": Path to a client key file for TLS. --cluster=\"\": The name of the kubeconfig cluster to use --context=\"\": The name of the kubeconfig context to use --insecure-skip-tls-verify[=false]: If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure. --kubeconfig=\"\": Path to the kubeconfig file to use for CLI requests. --log-backtrace-at=:0: when logging hits line file:N, emit a stack trace --log-dir=\"\": If non-empty, write log files in this directory --log-flush-frequency=5s: Maximum number of seconds between log flushes --logtostderr[=true]: log to standard error instead of files --match-server-version[=false]: Require server version to match client version --namespace=\"\": If present, the namespace scope for this CLI request. --password=\"\": Password for basic authentication to the API server. -s, --server=\"\": The address and port of the Kubernetes API server --stderrthreshold=2: logs at or above this threshold go to stderr --token=\"\": Bearer token for authentication to the API server. --user=\"\": The name of the kubeconfig user to use --username=\"\": Username for basic authentication to the API server. --v=0: log level for V logs --vmodule=: comma-separated list of pattern=N settings for file-filtered logging 2.3.3. kubectl客户端 命令行工具kubectl客户端，通过命令行参数转换为对API Server的REST API调用，并将调用结果输出。 命令格式：kubectl [command] [options] 具体可参考k8s常用命令 2.3.4. 编程方式调用 使用场景： 1、运行在Pod里的用户进程调用kubernetes API,通常用来实现分布式集群搭建的目标。 2、开发基于kubernetes的管理平台，比如调用kubernetes API来完成Pod、Service、RC等资源对象的图形化创建和管理界面。可以使用kubernetes提供的Client Library。 具体可参考https://github.com/kubernetes/client-go。 3. 通过API Server访问Node、Pod和Service k8s API Server最主要的REST接口是资源对象的增删改查，另外还有一类特殊的REST接口—k8s Proxy API接口，这类接口的作用是代理REST请求，即kubernetes API Server把收到的REST请求转发到某个Node上的kubelet守护进程的REST端口上，由该kubelet进程负责响应。 3.1. Node相关接口 关于Node相关的接口的REST路径为：/api/v1/proxy/nodes/{name}，其中{name}为节点的名称或IP地址。 /api/v1/proxy/nodes/{name}/pods/ #列出指定节点内所有Pod的信息 /api/v1/proxy/nodes/{name}/stats/ #列出指定节点内物理资源的统计信息 /api/v1/prxoy/nodes/{name}/spec/ #列出指定节点的概要信息 这里获取的Pod信息来自Node而非etcd数据库，两者时间点可能存在偏差。如果在kubelet进程启动时加--enable-debugging-handles=true参数，那么kubernetes Proxy API还会增加以下接口： /api/v1/proxy/nodes/{name}/run #在节点上运行某个容器 /api/v1/proxy/nodes/{name}/exec #在节点上的某个容器中运行某条命令 /api/v1/proxy/nodes/{name}/attach #在节点上attach某个容器 /api/v1/proxy/nodes/{name}/portForward #实现节点上的Pod端口转发 /api/v1/proxy/nodes/{name}/logs #列出节点的各类日志信息 /api/v1/proxy/nodes/{name}/metrics #列出和该节点相关的Metrics信息 /api/v1/proxy/nodes/{name}/runningpods #列出节点内运行中的Pod信息 /api/v1/proxy/nodes/{name}/debug/pprof #列出节点内当前web服务的状态，包括CPU和内存的使用情况 3.2. Pod相关接口 /api/v1/proxy/namespaces/{namespace}/pods/{name}/{path:*} #访问pod的某个服务接口 /api/v1/proxy/namespaces/{namespace}/pods/{name} #访问Pod #以下写法不同，功能一样 /api/v1/namespaces/{namespace}/pods/{name}/proxy/{path:*} #访问pod的某个服务接口 /api/v1/namespaces/{namespace}/pods/{name}/proxy #访问Pod 3.3. Service相关接口 /api/v1/proxy/namespaces/{namespace}/services/{name} Pod的proxy接口的作用：在kubernetes集群之外访问某个pod容器的服务（HTTP服务），可以用Proxy API实现，这种场景多用于管理目的，比如逐一排查Service的Pod副本，检查哪些Pod的服务存在异常问题。 4. 集群功能模块之间的通信 kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信，集群内各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，通过API Server提供的REST接口（GET/LIST/WATCH方法）来实现，从而实现各模块之间的信息交互。 4.1. kubelet与API Server交互 每个Node节点上的kubelet定期就会调用API Server的REST接口报告自身状态，API Server接收这些信息后，将节点状态信息更新到etcd中。kubelet也通过API Server的Watch接口监听Pod信息，从而对Node机器上的POD进行管理。 监听信息 kubelet动作 备注 新的POD副本被调度绑定到本节点 执行POD对应的容器的创建和启动逻辑 - POD对象被删除 删除本节点上相应的POD容器 - 修改POD信息 修改本节点的POD容器 - 4.2. kube-controller-manager与API Server交互 kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口，实时监控Node的信息，并做相应处理。 4.3. kube-scheduler与API Server交互 Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，它会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑。调度成功后将Pod绑定到目标节点上。 4.4. 特别说明 为了缓解各模块对API Server的访问压力，各功能模块都采用缓存机制来缓存数据，各功能模块定时从API Server获取指定的资源对象信息（LIST/WATCH方法），然后将信息保存到本地缓存，功能模块在某些情况下不直接访问API Server，而是通过访问缓存数据来间接访问API Server。 参考《kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"principle/kubernetes-core-principle-controller-manager.html":{"url":"principle/kubernetes-core-principle-controller-manager.html","title":"Controller Manager","keywords":"","body":"1. Controller Manager简介 Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。 图片 - controller manager 每个Controller通过API Server提供的接口实时监控整个集群的每个资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将系统状态修复到“期望状态”。 2. Replication Controller 为了区分，将资源对象Replication Controller简称RC,而本文中是指Controller Manager中的Replication Controller，称为副本控制器。副本控制器的作用即保证集群中一个RC所关联的Pod副本数始终保持预设值。 只有当Pod的重启策略是Always的时候（RestartPolicy=Always），副本控制器才会管理该Pod的操作（创建、销毁、重启等）。 RC中的Pod模板就像一个模具，模具制造出来的东西一旦离开模具，它们之间就再没关系了。一旦Pod被创建，无论模板如何变化，也不会影响到已经创建的Pod。 Pod可以通过修改label来脱离RC的管控，该方法可以用于将Pod从集群中迁移，数据修复等调试。 删除一个RC不会影响它所创建的Pod，如果要删除Pod需要将RC的副本数属性设置为0。 不要越过RC创建Pod，因为RC可以实现自动化控制Pod，提高容灾能力。 2.1. Replication Controller的职责 确保集群中有且仅有N个Pod实例，N是RC中定义的Pod副本数量。 通过调整RC中的spec.replicas属性值来实现系统扩容或缩容。 通过改变RC中的Pod模板来实现系统的滚动升级。 2.2. Replication Controller使用场景 使用场景 说明 使用命令 重新调度 当发生节点故障或Pod被意外终止运行时，可以重新调度保证集群中仍然运行指定的副本数。 弹性伸缩 通过手动或自动扩容代理修复副本控制器的spec.replicas属性，可以实现弹性伸缩。 kubectl scale 滚动更新 创建一个新的RC文件，通过kubectl 命令或API执行，则会新增一个新的副本同时删除旧的副本，当旧副本为0时，删除旧的RC。 kubectl rolling-update 滚动升级，具体可参考kubectl rolling-update --help,官方文档：https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/ 3. Node Controller kubelet在启动时会通过API Server注册自身的节点信息，并定时向API Server汇报状态信息，API Server接收到信息后将信息更新到etcd中。 Node Controller通过API Server实时获取Node的相关信息，实现管理和监控集群中的各个Node节点的相关控制功能。流程如下 图片 - Node Controller 1、Controller Manager在启动时如果设置了--cluster-cidr参数，那么为每个没有设置Spec.PodCIDR的Node节点生成一个CIDR地址，并用该CIDR地址设置节点的Spec.PodCIDR属性，防止不同的节点的CIDR地址发生冲突。 2、具体流程见以上流程图。 3、逐个读取节点信息，如果节点状态变成非“就绪”状态，则将节点加入待删除队列，否则将节点从该队列删除。 4. ResourceQuota Controller 资源配额管理确保指定的资源对象在任何时候都不会超量占用系统物理资源。 支持三个层次的资源配置管理： 1）容器级别：对CPU和Memory进行限制 2）Pod级别：对一个Pod内所有容器的可用资源进行限制 3）Namespace级别：包括 Pod数量 Replication Controller数量 Service数量 ResourceQuota数量 Secret数量 可持有的PV（Persistent Volume）数量 说明： k8s配额管理是通过Admission Control（准入控制）来控制的； Admission Control提供两种配额约束方式：LimitRanger和ResourceQuota； LimitRanger作用于Pod和Container； ResourceQuota作用于Namespace上，限定一个Namespace里的各类资源的使用总额。 ResourceQuota Controller流程图： 图片 - ResourceQuota Controller 5. Namespace Controller 用户通过API Server可以创建新的Namespace并保存在etcd中，Namespace Controller定时通过API Server读取这些Namespace信息。 如果Namespace被API标记为优雅删除（即设置删除期限，DeletionTimestamp）,则将该Namespace状态设置为“Terminating”,并保存到etcd中。同时Namespace Controller删除该Namespace下的ServiceAccount、RC、Pod等资源对象。 6. Endpoint Controller Service、Endpoint、Pod的关系： 图片 - Endpoint Controller Endpoints表示了一个Service对应的所有Pod副本的访问地址，而Endpoints Controller负责生成和维护所有Endpoints对象的控制器。它负责监听Service和对应的Pod副本的变化。 如果监测到Service被删除，则删除和该Service同名的Endpoints对象； 如果监测到新的Service被创建或修改，则根据该Service信息获得相关的Pod列表，然后创建或更新Service对应的Endpoints对象。 如果监测到Pod的事件，则更新它对应的Service的Endpoints对象。 kube-proxy进程获取每个Service的Endpoints，实现Service的负载均衡功能。 7. Service Controller Service Controller是属于kubernetes集群与外部的云平台之间的一个接口控制器。Service Controller监听Service变化，如果是一个LoadBalancer类型的Service，则确保外部的云平台上对该Service对应的LoadBalancer实例被相应地创建、删除及更新路由转发表。 参考《Kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"principle/kubernetes-core-principle-scheduler.html":{"url":"principle/kubernetes-core-principle-scheduler.html","title":"Scheduler","keywords":"","body":"1. Scheduler简介 Scheduler负责Pod调度。在整个系统中起\"承上启下\"作用，承上：负责接收Controller Manager创建的新的Pod，为其选择一个合适的Node；启下：Node上的kubelet接管Pod的生命周期。 Scheduler： 1）通过调度算法为待调度Pod列表的每个Pod从Node列表中选择一个最适合的Node，并将信息写入etcd中 2）kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载Image，并启动容器。 图片 - scheduler 2. 调度流程 1、预选调度过程，即遍历所有目标Node,筛选出符合要求的候选节点，kubernetes内置了多种预选策略（xxx Predicates）供用户选择 2、确定最优节点，在第一步的基础上采用优选策略（xxx Priority）计算出每个候选节点的积分，取最高积分。 调度流程通过插件式加载的“调度算法提供者”（AlgorithmProvider）具体实现，一个调度算法提供者就是包括一组预选策略与一组优选策略的结构体。 3. 预选策略 说明：返回true表示该节点满足该Pod的调度条件；返回false表示该节点不满足该Pod的调度条件。 3.1. NoDiskConflict 判断备选Pod的数据卷是否与该Node上已存在Pod挂载的数据卷冲突，如果是则返回false，否则返回true。 3.2. PodFitsResources 判断备选节点的资源是否满足备选Pod的需求，即节点的剩余资源满不满足该Pod的资源使用。 计算备选Pod和节点中已用资源（该节点所有Pod的使用资源）的总和。 获取备选节点的状态信息，包括节点资源信息。 如果（备选Pod+节点已用资源>该节点总资源）则返回false，即剩余资源不满足该Pod使用；否则返回true。 3.3. PodSelectorMatches 判断节点是否包含备选Pod的标签选择器指定的标签，即通过标签来选择Node。 如果Pod中没有指定spec.nodeSelector，则返回true。 否则获得备选节点的标签信息，判断该节点的标签信息中是否包含该Pod的spec.nodeSelector中指定的标签，如果包含返回true，否则返回false。 3.4. PodFitsHost 判断备选Pod的spec.nodeName所指定的节点名称与备选节点名称是否一致，如果一致返回true，否则返回false。 3.5. CheckNodeLabelPresence 检查备选节点中是否有Scheduler配置的标签，如果有返回true，否则返回false。 3.6. CheckServiceAffinity 判断备选节点是否包含Scheduler配置的标签，如果有返回true，否则返回false。 3.7. PodFitsPorts 判断备选Pod所用的端口列表中的端口是否在备选节点中已被占用，如果被占用返回false，否则返回true。 4. 优选策略 4.1. LeastRequestedPriority 优先从备选节点列表中选择资源消耗最小的节点（CPU+内存）。 4.2. CalculateNodeLabelPriority 优先选择含有指定Label的节点。 4.3. BalancedResourceAllocation 优先从备选节点列表中选择各项资源使用率最均衡的节点。 参考《Kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"principle/kubernetes-core-principle-kubelet.html":{"url":"principle/kubernetes-core-principle-kubelet.html","title":"Kubelet","keywords":"","body":"1. kubelet简介 在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点资源。可以把kubelet理解成【Server-Agent】架构中的agent，是Node上的pod管家。 更多kubelet配置参数信息可参考kubelet --help 2. 节点管理 节点通过设置kubelet的启动参数“--register-node”，来决定是否向API Server注册自己，默认为true。可以通过kubelet --help或者查看kubernetes源码【cmd/kubelet/app/server.go中】来查看该参数。 kubelet的配置文件 默认配置文件在/etc/kubernetes/kubelet中，其中 --api-servers：用来配置Master节点的IP和端口。 --kubeconfig：用来配置kubeconfig的路径，kubeconfig文件常用来指定证书。 --hostname-override：用来配置该节点在集群中显示的主机名。 --node-status-update-frequency：配置kubelet向Master心跳上报的频率，默认为10s。 3. Pod管理 kubelet有几种方式获取自身Node上所需要运行的Pod清单。但本文只讨论通过API Server监听etcd目录，同步Pod列表的方式。 kubelet通过API Server Client使用WatchAndList的方式监听etcd中/registry/nodes/${当前节点名称}和/registry/pods的目录，将获取的信息同步到本地缓存中。 kubelet监听etcd，执行对Pod的操作，对容器的操作则是通过Docker Client执行，例如启动删除容器等。 kubelet创建和修改Pod流程： 为该Pod创建一个数据目录。 从API Server读取该Pod清单。 为该Pod挂载外部卷（External Volume） 下载Pod用到的Secret。 检查运行的Pod，执行Pod中未完成的任务。 先创建一个Pause容器，该容器接管Pod的网络，再创建其他容器。 Pod中容器的处理流程： 1）比较容器hash值并做相应处理。 2）如果容器被终止了且没有指定重启策略，则不做任何处理。 3）调用Docker Client下载容器镜像，调用Docker Client运行容器。 4. 容器健康检查 Pod通过探针的方式来检查容器的健康状态，具体可参考Pod详解#Pod健康检查。 5. cAdvisor资源监控 kubelet通过cAdvisor获取本节点信息及容器的数据。cAdvisor为谷歌开源的容器资源分析工具，默认集成到kubernetes中。 cAdvisor自动采集CPU,内存，文件系统，网络使用情况，容器中运行的进程，默认端口为4194。可以通过Node IP+Port访问。 更多参考：http://github.com/google/cadvisor 参考《Kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"operation/kubernetes-troubleshooting.html":{"url":"operation/kubernetes-troubleshooting.html","title":"kubernetes集群问题排查","keywords":"","body":"1. 查看系统Event事件 kubectl describe pod --namespace= 该命令可以显示Pod创建时的配置定义、状态等信息和最近的Event事件，事件信息可用于排错。例如当Pod状态为Pending，可通过查看Event事件确认原因，一般原因有几种： 没有可用的Node可调度 开启了资源配额管理并且当前Pod的目标节点上恰好没有可用的资源 正在下载镜像（镜像拉取耗时太久）或镜像下载失败。 kubectl describe还可以查看其它k8s对象：NODE,RC,Service,Namespace,Secrets。 1.1. Pod kubectl describe pod --namespace= 以下是容器的启动命令非阻塞式导致容器挂掉，被k8s频繁重启所产生的事件。 kubectl describe pod --namespace= Events: FirstSeen LastSeen Count From SubobjectPath Reason Message ───────── ──────── ───── ──── ───────────── ────── ─────── 7m 7m 1 {scheduler } Scheduled Successfully assigned yangsc-1-0-0-index0 to 10.8.216.19 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Pulled Container image \"gcr.io/kube-system/pause:0.8.0\" already present on machine 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Created Created with docker id 84f133c324d0 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Started Started with docker id 84f133c324d0 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 3f9f82abb145 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 3f9f82abb145 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id fb112e4002f4 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id fb112e4002f4 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 613b119d4474 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 613b119d4474 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 25cb68d1fd3d 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 25cb68d1fd3d 5m 5m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 7d9ee8610b28 5m 5m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 7d9ee8610b28 3m 3m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 88b9e8d582dd 3m 3m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 88b9e8d582dd 7m 1m 7 {kubelet 10.8.216.19} containers{yangsc0} Pulling Pulling image \"gcr.io/test/tcp-hello:1.0.0\" 1m 1m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 089abff050e7 1m 1m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 089abff050e7 7m 1m 7 {kubelet 10.8.216.19} containers{yangsc0} Pulled Successfully pulled image \"gcr.io/test/tcp-hello:1.0.0\" 6m 7s 34 {kubelet 10.8.216.19} containers{yangsc0} Backoff Back-off restarting failed docker container 1.2. NODE kubectl describe node 10.8.216.20 [root@FC-43745A-10 ~]# kubectl describe node 10.8.216.20 Name: 10.8.216.20 Labels: kubernetes.io/hostname=10.8.216.20,namespace/bcs-cc=true,namespace/myview=true CreationTimestamp: Mon, 17 Apr 2017 11:32:52 +0800 Phase: Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ──── ────── ───────────────── ────────────────── ────── ─────── Ready True Fri, 18 Aug 2017 09:38:33 +0800 Tue, 02 May 2017 17:40:58 +0800 KubeletReady kubelet is posting ready status OutOfDisk False Fri, 18 Aug 2017 09:38:33 +0800 Mon, 17 Apr 2017 11:31:27 +0800 KubeletHasSufficientDisk kubelet has sufficient disk space available Addresses: 10.8.216.20,10.8.216.20 Capacity: cpu: 32 memory: 67323039744 pods: 40 System Info: Machine ID: 723bafc7f6764022972b3eae1ce6b198 System UUID: 4C4C4544-0042-4210-8044-C3C04F595631 Boot ID: da01f2e3-987a-425a-9ca7-1caaec35d1e5 Kernel Version: 3.10.0-327.28.3.el7.x86_64 OS Image: CentOS Linux 7 (Core) Container Runtime Version: docker://1.13.1 Kubelet Version: v1.1.1-xxx2-13.1+79c90c68bfb72f-dirty Kube-Proxy Version: v1.1.1-xxx2-13.1+79c90c68bfb72f-dirty ExternalID: 10.8.216.20 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits ───────── ──── ──────────── ────────── ─────────────── ───────────── bcs-cc bcs-cc-api-0-0-1364-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) bcs-cc bcs-cc-api-0-0-1444-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) fw fw-demo2-0-0-1519-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) myview myview-api-0-0-1362-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) myview myview-api-0-0-1442-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) qa-ts-dna ts-dna-console3-0-0-1434-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) Allocated resources: (Total limits may be over 100%, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md) CPU Requests CPU Limits Memory Requests Memory Limits ──────────── ────────── ─────────────── ───────────── 6 (18%) 6 (18%) 25769803776 (38%) 25769803776 (38%) No events. 1.3. RC kubectl describe rc mytest-1-0-0 --namespace=test [root@FC-43745A-10 ~]# kubectl describe rc mytest-1-0-0 --namespace=test Name: mytest-1-0-0 Namespace: test Image(s): gcr.io/test/mywebcalculator:1.0.1 Selector: app=mytest,appVersion=1.0.0 Labels: app=mytest,appVersion=1.0.0,env=ts,zone=inner Replicas: 1 current / 1 desired Pods Status: 1 Running / 0 Waiting / 0 Succeeded / 0 Failed No volumes. Events: FirstSeen LastSeen Count From SubobjectPath Reason Message ───────── ──────── ───── ──── ───────────── ────── ─────── 20h 19h 9 {replication-controller } FailedCreate Error creating: Pod \"mytest-1-0-0-index0\" is forbidden: limited to 10 pods 20h 17h 7 {replication-controller } FailedCreate Error creating: pods \"mytest-1-0-0-index0\" already exists 20h 17h 4 {replication-controller } SuccessfulCreate Created pod: mytest-1-0-0-index0 1.4. NAMESPACE kubectl describe namespace test [root@FC-43745A-10 ~]# kubectl describe namespace test Name: test Labels: Status: Active Resource Quotas Resource Used Hard --- --- --- cpu 5 20 memory 1342177280 53687091200 persistentvolumeclaims 0 10 pods 4 10 replicationcontrollers 8 20 resourcequotas 1 1 secrets 3 10 services 8 20 No resource limits. 1.5. Service kubectl describe service xxx-containers-1-1-0 --namespace=test [root@FC-43745A-10 ~]# kubectl describe service xxx-containers-1-1-0 --namespace=test Name: xxx-containers-1-1-0 Namespace: test Labels: app=xxx-containers,appVersion=1.1.0,env=ts,zone=inner Selector: app=xxx-containers,appVersion=1.1.0 Type: ClusterIP IP: 10.254.46.42 Port: port-dna-tcp-35913 35913/TCP Endpoints: 10.0.92.17:35913 Port: port-l7-tcp-8080 8080/TCP Endpoints: 10.0.92.17:8080 Session Affinity: None No events. 2. 查看容器日志 1、查看指定pod的日志 kubectl logs kubectl logs -f #类似tail -f的方式查看 2、查看上一个pod的日志 kubectl logs -p 3、查看指定pod中指定容器的日志 kubectl logs -c 4、kubectl logs --help [root@node5 ~]# kubectl logs --help Print the logs for a container in a pod. If the pod has only one container, the container name is optional. Usage: kubectl logs [-f] [-p] POD [-c CONTAINER] [flags] Aliases: logs, log Examples: # Return snapshot logs from pod nginx with only one container $ kubectl logs nginx # Return snapshot of previous terminated ruby container logs from pod web-1 $ kubectl logs -p -c ruby web-1 # Begin streaming the logs of the ruby container in pod web-1 $ kubectl logs -f -c ruby web-1 # Display only the most recent 20 lines of output in pod nginx $ kubectl logs --tail=20 nginx # Show all logs from pod nginx written in the last hour $ kubectl logs --since=1h nginx 3. 查看k8s服务日志 3.1. journalctl 在Linux系统上systemd系统来管理kubernetes服务，并且journal系统会接管服务程序的输出日志，可以通过systemctl status 或journalctl -u -f来查看kubernetes服务的日志。 其中kubernetes组件包括： k8s组件 涉及日志内容 备注 kube-apiserver kube-controller-manager Pod扩容相关或RC相关 kube-scheduler Pod扩容相关或RC相关 kubelet Pod生命周期相关：创建、停止等 etcd 3.2. 日志文件 也可以通过指定日志存放目录来保存和查看日志 --logtostderr=false：不输出到stderr --log-dir=/var/log/kubernetes:日志的存放目录 --alsologtostderr=false:设置为true表示日志输出到文件也输出到stderr --v=0:glog的日志级别 --vmodule=gfs=2,test=4：glog基于模块的详细日志级别 4. 常见问题 4.1. Pod状态一直为Pending kubectl describe --namespace= 查看该POD的事件。 正在下载镜像但拉取不下来（镜像拉取耗时太久）[一般都是该原因] 没有可用的Node可调度 开启了资源配额管理并且当前Pod的目标节点上恰好没有可用的资源 解决方法： 查看该POD所在宿主机与镜像仓库之间的网络是否有问题，可以手动拉取镜像 删除POD实例，让POD调度到别的宿主机上 4.2. Pod创建后不断重启 kubectl get pods中Pod状态一会running，一会不是，且RESTARTS次数不断增加。 一般原因为容器启动命令不是阻塞式命令，导致容器运行后马上退出。 非阻塞式命令： 本身CMD指定的命令就是非阻塞式命令 将服务启动方式设置为后台运行 解决方法： 1、将命令改为阻塞式命令（前台运行），例如：zkServer.sh start-foreground 2、java运行程序的启动脚本将 nohup xxx &的nobup和&去掉，例如： nohup JAVA_HOME/bin/java JAVA_OPTS -cp $CLASSPATH com.cnc.open.processor.Main & 改为： JAVA_HOME/bin/java JAVA_OPTS -cp $CLASSPATH com.cnc.open.processor.Main 文章参考《Kubernetes权威指南》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"operation/nodeselector-and-taint.html":{"url":"operation/nodeselector-and-taint.html","title":"指定Node调度与隔离","keywords":"","body":"1. NodeSelector 1.1. 概念 如果需要限制Pod到指定的Node上运行，则可以给Node打标签并给Pod配置NodeSelector。 1.2. 使用方式 1.2.1. 给Node打标签 # get node的name kubectl get nodes # 设置Label kubectl label nodes = # 例如 kubectl label nodes node-1 disktype=ssd # 查看Node的Label kubectl get nodes --show-labels 1.2.2. 给Pod设置NodeSelector apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd # 对应Node的Label 1.3. 亲和性（Affinity）和反亲和性（Anti-affinity） 待补充 2. Taint 和 Toleration 2.1. 概念 nodeSelector可以通过打标签的形式让Pod被调度到指定的Node上，Taint则相反，它使节点能够排斥一类特定的Pod，除非Pod被指定了toleration的标签。（taint即污点，Node被打上污点；只有容忍[toleration]这些污点的Pod才可能被调度到该Node）。 2.2. 使用方式 2.2.1. kubectl taint # 给节点增加一个taint，它的key是，value是，effect是NoSchedule。 kubectl taint nodes =:NoSchedule 只有拥有和这个taint相匹配的toleration的pod才能够被分配到 node_name 这个节点。 例如，在 PodSpec 中定义 pod 的 toleration： tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\" tolerations: - key: \"key\" operator: \"Exists\" effect: \"NoSchedule\" 2.2.2. 匹配规则： 一个 toleration 和一个 taint 相“匹配”是指它们有一样的 key 和 effect ，并且： 如果 operator 是 Exists （此时 toleration 不能指定 value） 如果 operator 是 Equal ，则它们的 value 应该相等 特殊情况： 如果一个 toleration 的 key 为空且 operator 为 Exists ，表示这个 toleration 与任意的 key 、 value 和 effect 都匹配，即这个 toleration 能容忍任意 taint。 tolerations: - operator: \"Exists\" 如果一个 toleration 的 effect 为空，则 key 值与之相同的相匹配 taint 的 effect 可以是任意值。 tolerations: - key: \"key\" operator: \"Exists\" 一个节点可以设置多个taint，一个pod也可以设置多个toleration。Kubernetes 处理多个 taint 和 toleration 的过程就像一个过滤器：从一个节点的所有 taint 开始遍历，过滤掉那些 pod 中存在与之相匹配的 toleration 的 taint。余下未被过滤的 taint 的 effect 值决定了 pod 是否会被分配到该节点，特别是以下情况： 如果未被过滤的 taint 中存在一个以上 effect 值为 NoSchedule 的 taint，则 Kubernetes 不会将 pod 分配到该节点。 如果未被过滤的 taint 中不存在 effect 值为 NoSchedule 的 taint，但是存在 effect 值为 PreferNoSchedule 的 taint，则 Kubernetes 会尝试将 pod 分配到该节点。 如果未被过滤的 taint 中存在一个以上 effect 值为 NoExecute 的 taint，则 Kubernetes 不会将 pod 分配到该节点（如果 pod 还未在节点上运行），或者将 pod 从该节点驱逐（如果 pod 已经在节点上运行）。 2.2.3. effect的类型 NoSchedule：只有拥有和这个 taint 相匹配的 toleration 的 pod 才能够被分配到这个节点。 PreferNoSchedule：系统会尽量避免将 pod 调度到存在其不能容忍 taint 的节点上，但这不是强制的。 NoExecute ：任何不能忍受这个 taint 的 pod 都会马上被驱逐，任何可以忍受这个 taint 的 pod 都不会被驱逐。Pod可指定属性 tolerationSeconds 的值，表示pod 还能继续在节点上运行的时间。 tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" tolerationSeconds: 3600 2.3. 使用场景 2.3.1. 专用节点 kubectl taint nodes dedicated=:NoSchedule 先给Node添加taint，然后给Pod添加相对应的 toleration，则该Pod可调度到taint的Node，也可调度到其他节点。 如果想让Pod只调度某些节点且某些节点只接受对应的Pod，则需要在Node上添加Label（例如：dedicated=groupName），同时给Pod的nodeSelector添加对应的Label。 2.3.2. 特殊硬件节点 如果某些节点配置了特殊硬件（例如CPU），希望不使用这些特殊硬件的Pod不被调度该Node，以便保留必要资源。即可给Node设置taint和label，同时给Pod设置toleration和label来使得这些Node专门被指定Pod使用。 # kubectl taint kubectl taint nodes nodename special=true:NoSchedule # 或者 kubectl taint nodes nodename special=true:PreferNoSchedule 2.3.3. 基于taint驱逐 effect 值 NoExecute ，它会影响已经在节点上运行的 pod，即根据策略对Pod进行驱逐。 如果 pod 不能忍受effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐 如果 pod 能够忍受effect 值为 NoExecute 的 taint，但是在 toleration 定义中没有指定 tolerationSeconds，则 pod 还会一直在这个节点上运行。 如果 pod 能够忍受effect 值为 NoExecute 的 taint，而且指定了 tolerationSeconds，则 pod 还能在这个节点上继续运行这个指定的时间长度。 参考： https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"monitor/kubernetes-cluster-monitoring.html":{"url":"monitor/kubernetes-cluster-monitoring.html","title":"监控体系介绍","keywords":"","body":"1. 概述 1.1. cAdvisor cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，cAdvisor集成在Kubelet中，当kubelet启动时会自动启动cAdvisor，即一个cAdvisor仅对一台Node机器进行监控。kubelet的启动参数--cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器Node_IP:port访问。项目主页：http://github.com/google/cadvisor。 1.2. Heapster 是对集群中的各个Node、Pod的资源使用数据进行采集，通过访问每个Node上Kubelet的API，再通过Kubelet调用cAdvisor的API来采集该节点上所有容器的性能数据。由Heapster进行数据汇聚，保存到后端存储系统中，例如InfluxDB，Google Cloud Logging等。项目主页为：https://github.com/kubernetes/heapster。 1.3. InfluxDB 是分布式时序数据库（每条记录带有时间戳属性），主要用于实时数据采集、事件跟踪记录、存储时间图表、原始数据等。提供REST API用于数据的存储和查询。项目主页为http://InfluxDB.com。 1.4. Grafana 通过Dashboard将InfluxDB的时序数据展现成图表形式，便于查看集群运行状态。项目主页为http://Grafana.org。 1.5. 总体架构图 图片 - k8s监控架构图 其中当前Kubernetes中，Heapster、InfluxDB、Grafana均以Pod的形式启动和运行。Heapster与Master需配置安全连接。 2. 部署与使用 2.1. cAdvisor kubelet的启动参数--cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器Node_IP:port访问。也提供了REST API供客户端远程调用，API返回的格式为JSON，可以采用URL访问：http://`hostname`:`port`/api/`version`/`request`/ 例如：http://14.152.49.100:4194/api/v1.3/machine 获取主机信息。 2.2. Service 2.2.1. heapster-service heapster-service.yaml apiVersion:v1 kind:Service metadata: label: kubenetes.io/cluster-service:\"true\" kubernetes.io/name:Heapster name:heapster namespace:kube-system spec: ports: - port:80 targetPort:8082 selector: k8s-app:heapster 2.2.2. influxdb-service influxdb-service.yaml apiVersion:v1 kind:Service metadata: label:null name:monitoring-InfluxDB namespace:kube-system spec: type:Nodeport ports: - name:http port:80 targetPort:8083 - name:api port:8086 targetPort:8086 Nodeport:8086 selector: name:influxGrafana 2.2.3. grafana-service grafana-service.yaml apiVersion:v1 kind:Service metadata: label: kubenetes.io/cluster-service:\"true\" kubernetes.io/name:monitoring-Grafana name:monitoring-Grafana namespace:kube-system spec: type:Nodeport ports: port:80 targetPort:8080 Nodeport:8085 selector: name:influxGrafana 使用type=NodePort将InfluxDB和Grafana暴露在Node的端口上，以便通过浏览器进行访问。 2.2.4. 创建service kubectl create -f heapster-service.yaml kubectl create -f InfluxDB-service.yaml kubectl create -f Grafana-service.yaml 2.3. ReplicationController 2.3.1. influxdb-grafana-controller influxdb-grafana-controller-v3.yaml apiVersion:v1 kind:ReplicationController metadata: name:monitoring-influxdb-grafana-v3 namespace:kube-system labels: k8s-app:influxGrafana version:v3 kubernetes.io/cluster-service:\"true spec: replicas:1 selector: k8s-app:influxGrafana version:v3 template: metadata: labels: k8s-app:influxGrafana version:v3 kubernetes.io/cluster-service:\"true spec: containers: - image:gcr.io/google_containers/heapster_influxdb:v0.5 name:influxdb resources: limits: cpu:100m memory:500Mi requests: cpu:100m memory:500Mi ports: - containerPort:8083 - containerPort:8086 volumeMounts: -name:influxdb-persistent-storage mountPath:/data - image:grc.io/google_containers/heapster_grafana:v2.6.0-2 name:grafana resources: limits: cpu:100m memory:100Mi requests: cpu:100m memory:100Mi env: - name:INFLUXDB_SERVICE_URL value:http://monitoring-influxdb:8086 - name:GF_AUTH_BASIC_ENABLED value:\"false\" - name:GF_AUTH_ANONYMOUS_ENABLED value:\"true\" - name:GF_AUTH_ANONYMOUS_ORG_ROLE value:Admin - name:GF_SERVER_ROOT_URL value:/api/v1/proxy/namespace/kube-system/services/monitoring-grafana/ volumeMounts: - name:grafana-persistent-storage mountPath:/var volumes: - name:influxdb-persistent-storage emptyDir{} - name:grafana-persistent-storage emptyDir{} 2.3.2. heapster-controller heapster-controller.yaml apiVersion:v1 kind:ReplicationController metadata: labels: k8s-app:heapster name:heapster version:v6 name:heapster namespace:kube-system spec: replicas:1 selector: name:heapster k8s-app:heapster version:v6 template: metadata: labels: k8s-app:heapster version:v6 spec: containers: - image:gcr.io/google_containers/heapster:v0.17.0 name:heapster command: - /heapster - --source=kubernetes:http://192.168.1.128:8080?inClusterConfig=flase&kubeletHttps=true&useServiceAccount=true&auth= - --sink=InfluxDB:http://monitoring-InfluxDB:8086 Heapster设置启动参数说明： 1、–source 配置监控来源，本例中表示从k8s-Master获取各个Node的信息。在URL的参数部分，修改kubeletHttps、inClusterConfig、useServiceAccount的值。 2、–sink 配置后端的存储系统，本例中使用InfluxDB。URL中主机名的地址是InfluxDB的Service名字，需要DNS服务正常工作，如果没有配置DNS服务可使用Service的ClusterIP地址。 2.3.3. 创建ReplicationController kubelet create -f InfluxDB-Grafana-controller.yaml kubelet create -f heapster-controller.yaml 3. 查看界面及数据 3.1. InfluxDB 访问任意一台Node机器的30083端口。 3.2. Grafana 访问任意一台Node机器的30080端口。 4. 容器化部署 4.1. 拉取镜像 docker pull influxdb:latest docker pull cadvisor:latest docker pull grafana:latest docker pull heapster:latest 4.2. 运行容器 4.2.1. influxdb #influxdb docker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --volume=/opt/data/influxdb:/data --name influxsrv influxdb:latest 4.2.2. cadvisor #cadvisor docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --link influxsrv:influxsrv --name=cadvisor cadvisor:latest -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 4.2.3. grafana #grafana docker run -d -p 3000:3000 -e INFLUXDB_HOST=influxsrv -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=cadvisor -e INFLUXDB_USER=root -e INFLUXDB_PASS=root --link influxsrv:influxsrv --name grafana grafana:latest 4.2.4. heapster docker run -d -p 8082:8082 --net=host heapster:canary --source=kubernetes:http://`k8s-server-ip`:8080?inClusterConfig=false/&useServiceAccount=false --sink=influxdb:http://`influxdb-ip`:8086 4.3. 访问 在浏览器输入IP:PORT Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"monitor/cadvisor-introduction.html":{"url":"monitor/cadvisor-introduction.html","title":"cAdvisor介绍","keywords":"","body":"1. cAdvisor简介 ​ cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，cAdvisor集成在Kubelet中，当kubelet启动时会自动启动cAdvisor，即一个cAdvisor仅对一台Node机器进行监控。kubelet的启动参数--cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器访问。项目主页：http://github.com/google/cadvisor。 2. cAdvisor结构图 图片 - cAdvisor 3. Metrics 分类 字段 描述 cpu cpu_usage_total cpu_usage_system cpu_usage_user cpu_usage_per_cpu load_average Smoothed average of number of runnable threads x 1000 memory memory_usage Memory Usage memory_working_set Working set size network rx_bytes Cumulative count of bytes received rx_errors Cumulative count of receive errors encountered tx_bytes Cumulative count of bytes transmitted tx_errors Cumulative count of transmit errors encountered filesystem fs_device Filesystem device fs_limit Filesystem limit fs_usage Filesystem usage 4. cAdvisor源码 4.1. cAdvisor入口函数 cadvisor.go func main() { defer glog.Flush() flag.Parse() if *versionFlag { fmt.Printf(\"cAdvisor version %s (%s)/n\", version.Info[\"version\"], version.Info[\"revision\"]) os.Exit(0) } setMaxProcs() memoryStorage, err := NewMemoryStorage() if err != nil { glog.Fatalf(\"Failed to initialize storage driver: %s\", err) } sysFs, err := sysfs.NewRealSysFs() if err != nil { glog.Fatalf(\"Failed to create a system interface: %s\", err) } collectorHttpClient := createCollectorHttpClient(*collectorCert, *collectorKey) containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, &collectorHttpClient) if err != nil { glog.Fatalf(\"Failed to create a Container Manager: %s\", err) } mux := http.NewServeMux() if *enableProfiling { mux.HandleFunc(\"/debug/pprof/\", pprof.Index) mux.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline) mux.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) mux.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) } // Register all HTTP handlers. err = cadvisorhttp.RegisterHandlers(mux, containerManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm) if err != nil { glog.Fatalf(\"Failed to register HTTP handlers: %v\", err) } cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, nil) // Start the manager. if err := containerManager.Start(); err != nil { glog.Fatalf(\"Failed to start container manager: %v\", err) } // Install signal handler. installSignalHandler(containerManager) glog.Infof(\"Starting cAdvisor version: %s-%s on port %d\", version.Info[\"version\"], version.Info[\"revision\"], *argPort) addr := fmt.Sprintf(\"%s:%d\", *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux)) } 核心代码： memoryStorage, err := NewMemoryStorage() sysFs, err := sysfs.NewRealSysFs() #创建containerManager containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, &collectorHttpClient) #启动containerManager err := containerManager.Start() 4.2. cAdvisor Client的使用 import \"github.com/google/cadvisor/client\" func main(){ client, err := client.NewClient(\"http://192.168.19.30:4194/\") //http://:/ } 4.2.1 client定义 cadvisor/client/client.go // Client represents the base URL for a cAdvisor client. type Client struct { baseUrl string } // NewClient returns a new v1.3 client with the specified base URL. func NewClient(url string) (*Client, error) { if !strings.HasSuffix(url, \"/\") { url += \"/\" } return &Client{ baseUrl: fmt.Sprintf(\"%sapi/v1.3/\", url), }, nil } 4.2.2. client方法 1）MachineInfo // MachineInfo returns the JSON machine information for this client. // A non-nil error result indicates a problem with obtaining // the JSON machine information data. func (self *Client) MachineInfo() (minfo *v1.MachineInfo, err error) { u := self.machineInfoUrl() ret := new(v1.MachineInfo) if err = self.httpGetJsonData(ret, nil, u, \"machine info\"); err != nil { return } minfo = ret return } 2）ContainerInfo // ContainerInfo returns the JSON container information for the specified // container and request. func (self *Client) ContainerInfo(name string, query *v1.ContainerInfoRequest) (cinfo *v1.ContainerInfo, err error) { u := self.containerInfoUrl(name) ret := new(v1.ContainerInfo) if err = self.httpGetJsonData(ret, query, u, fmt.Sprintf(\"container info for %q\", name)); err != nil { return } cinfo = ret return } 3）DockerContainer // Returns the JSON container information for the specified // Docker container and request. func (self *Client) DockerContainer(name string, query *v1.ContainerInfoRequest) (cinfo v1.ContainerInfo, err error) { u := self.dockerInfoUrl(name) ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(&ret, query, u, fmt.Sprintf(\"Docker container info for %q\", name)); err != nil { return } if len(ret) != 1 { err = fmt.Errorf(\"expected to only receive 1 Docker container: %+v\", ret) return } for _, cont := range ret { cinfo = cont } return } 4）AllDockerContainers // Returns the JSON container information for all Docker containers. func (self *Client) AllDockerContainers(query *v1.ContainerInfoRequest) (cinfo []v1.ContainerInfo, err error) { u := self.dockerInfoUrl(\"/\") ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(&ret, query, u, \"all Docker containers info\"); err != nil { return } cinfo = make([]v1.ContainerInfo, 0, len(ret)) for _, cont := range ret { cinfo = append(cinfo, cont) } return } Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"monitor/heapster-introduction.html":{"url":"monitor/heapster-introduction.html","title":"Heapster介绍","keywords":"","body":"1. heapster简介 Heapster是容器集群监控和性能分析工具，天然的支持Kubernetes和CoreOS。 Kubernetes有个出名的监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor，它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。 2. heapster部署与配置 2.1. 注意事项 需同步部署机器和被采集机器的时间：ntpdate time.windows.com 加入定时任务，定期同步时间 crontab –e 30 5 * /usr/sbin/ntpdate time.windows.com //每天早晨5点半执行 2.2. 容器式部署 #拉取镜像 docker pull heapster:latest #运行容器 docker run -d -p 8082:8082 --net=host heapster:latest --source=kubernetes:http://:8080?inClusterConfig=false\\&useServiceAccount=false --sink=influxdb:http://:8086?db= 2.3. 配置说明 可以参考官方文档 2.3.1. –source –source: 指定数据获取源。这里指定kube-apiserver即可。 后缀参数： inClusterConfig: kubeletPort: 指定kubelet的使用端口，默认10255 kubeletHttps: 是否使用https去连接kubelets(默认：false) apiVersion: 指定K8S的apiversion insecure: 是否使用安全证书(默认：false) auth: 安全认证 useServiceAccount: 是否使用K8S的安全令牌 2.3.2. –sink –sink: 指定后端数据存储。这里指定influxdb数据库。 后缀参数： user: InfluxDB用户 pw: InfluxDB密码 db: 数据库名 secure: 安全连接到InfluxDB(默认：false) withfields： 使用InfluxDB fields(默认：false)。 3. Metrics 分类 Metric Name Description 备注 cpu cpu/limit CPU hard limit in millicores. CPU上限 cpu/node_capacity Cpu capacity of a node. Node节点的CPU容量 cpu/node_allocatable Cpu allocatable of a node. Node节点可分配的CPU cpu/node_reservation Share of cpu that is reserved on the node allocatable. cpu/node_utilization CPU utilization as a share of node allocatable. cpu/request CPU request (the guaranteed amount of resources) in millicores. cpu/usage Cumulative CPU usage on all cores. CPU总使用量 cpu/usage_rate CPU usage on all cores in millicores. filesystem filesystem/usage Total number of bytes consumed on a filesystem. 文件系统的使用量 filesystem/limit The total size of filesystem in bytes. 文件系统的使用上限 filesystem/available The number of available bytes remaining in a the filesystem 可用的文件系统容量 filesystem/inodes The number of available inodes in a the filesystem filesystem/inodes_free The number of free inodes remaining in a the filesystem memory memory/limit Memory hard limit in bytes. 内存上限 memory/major_page_faults Number of major page faults. memory/major_page_faults_rate Number of major page faults per second. memory/node_capacity Memory capacity of a node. memory/node_allocatable Memory allocatable of a node. memory/node_reservation Share of memory that is reserved on the node allocatable. memory/node_utilization Memory utilization as a share of memory allocatable. memory/page_faults Number of page faults. memory/page_faults_rate Number of page faults per second. memory/request Memory request (the guaranteed amount of resources) in bytes. memory/usage Total memory usage. memory/cache Cache memory usage. memory/rss RSS memory usage. memory/working_set Total working set usage. Working set is the memory being used and not easily dropped by the kernel. network network/rx Cumulative number of bytes received over the network. network/rx_errors Cumulative number of errors while receiving over the network. network/rx_errors_rate Number of errors while receiving over the network per second. network/rx_rate Number of bytes received over the network per second. network/tx Cumulative number of bytes sent over the network network/tx_errors Cumulative number of errors while sending over the network network/tx_errors_rate Number of errors while sending over the network network/tx_rate Number of bytes sent over the network per second. uptime Number of milliseconds since the container was started. - 4. Labels Label Name Description pod_id Unique ID of a Pod pod_name User-provided name of a Pod pod_namespace The namespace of a Pod container_base_image Base image for the container container_name User-provided name of the container or full cgroup name for system containers host_id Cloud-provider specified or user specified Identifier of a node hostname Hostname where the container ran labels Comma-separated(Default) list of user-provided labels. Format is 'key:value' namespace_id UID of the namespace of a Pod resource_id A unique identifier used to differentiate multiple metrics of the same type. e.x. Fs partitions under filesystem/usage 5. heapster API 见官方文档：https://github.com/kubernetes/heapster/blob/master/docs/model.md Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "},"monitor/influxdb-introduction.html":{"url":"monitor/influxdb-introduction.html","title":"Influxdb介绍","keywords":"","body":"1. InfluxDB简介 InfluxDB是一个当下比较流行的时序数据库，InfluxDB使用 Go 语言编写，无需外部依赖，安装配置非常方便，适合构建大型分布式系统的监控系统。 主要特色功能： 1）基于时间序列，支持与时间有关的相关函数（如最大，最小，求和等） 2）可度量性：你可以实时对大量数据进行计算 3）基于事件：它支持任意的事件数据 2. InfluxDB安装 1）安装 wget https://dl.influxdata.com/influxdb/releases/influxdb-0.13.0.x86_64.rpm yum localinstall influxdb-0.13.0.armhf.rpm 2）启动 service influxdb start 3）访问 http://服务器IP:8083 4）docker image方式安装 docker pull influxdb docker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --volume=/opt/data/influxdb:/data --name influxsrv influxdb:latest 3. InfluxDB的基本概念 3.1. 与传统数据库中的名词做比较 influxDB中的名词 传统数据库中的概念 database 数据库 measurement 数据库中的表 points 表里面的一行数据 3.2. InfluxDB中独有的概念 3.2.1. Point Point由时间戳（time）、数据（field）、标签（tags）组成。 Point相当于传统数据库里的一行数据，如下表所示： Point属性 传统数据库中的概念 time 每个数据记录时间，是数据库中的主索引(会自动生成) fields 各种记录值（没有索引的属性）也就是记录的值：温度， 湿度 tags 各种有索引的属性：地区，海拔 3.2.2. series 所有在数据库中的数据，都需要通过图表来展示，而这个series表示这个表里面的数据，可以在图表上画成几条线：通过tags排列组合算出来 show series from cpu 4. InfluxDB的基本操作 InfluxDB提供三种操作方式： 1）客户端命令行方式 2）HTTP API接口 3）各语言API库 4.1. InfluxDB数据库操作 操作 命令 显示数据库 show databases 创建数据库 create database db_name 删除数据库 drop database db_name 使用某个数据库 use db_name 4.2. InfluxDB数据表操作 操作 命令 说明 显示所有表 SHOW MEASUREMENTS 创建数据表 insert table_name,hostname=server01 value=442221834240i 1435362189575692182 其中 disk_free 就是表名，hostname是索引，value=xx是记录值，记录值可以有多个，最后是指定的时间 删除数据表 drop measurement table_name 查看表内容 select * from table_name 查看series show series from table_name series表示这个表里面的数据，可以在图表上画成几条线，series主要通过tags排列组合算出来 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2018-08-23 19:49:37 "}}